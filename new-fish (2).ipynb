{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11363619,"sourceType":"datasetVersion","datasetId":7112584}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport time\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, Subset\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.models import efficientnet_b0\nfrom PIL import Image\nimport random\nimport torchvision.transforms.functional as TF\nfrom tqdm.notebook import tqdm\nimport cv2\nimport zipfile\nfrom collections import Counter\nimport shutil\nimport os\nimport shutil\nfrom pathlib import Path\nfrom torchvision.utils import save_image\nfrom PIL import Image\nimport time\nimport copy\nfrom torchvision import models, transforms\nfrom torchvision.datasets import ImageFolder\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import accuracy_score\nimport time\nfrom datetime import datetime\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom itertools import cycle\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, Subset\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.models import efficientnet_b0\nimport torchvision.transforms.functional as TF\nfrom torchvision.models import ViT_B_16_Weights, vit_b_16\nfrom torchvision.models import vit_b_16, ViT_B_16_Weights\nfrom collections import OrderedDict\nimport copy\nimport torchvision.transforms.functional as TF\nimport torch.nn.functional as F\nimport cv2\nfrom torchvision import transforms, models\nfrom datetime import datetime\nfrom collections import Counter\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\nimport torchvision.utils as vutils\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom sklearn.preprocessing import label_binarize\nfrom itertools import cycle\nimport glob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T15:37:35.065643Z","iopub.execute_input":"2025-04-16T15:37:35.066294Z","iopub.status.idle":"2025-04-16T15:37:46.995111Z","shell.execute_reply.started":"2025-04-16T15:37:35.066268Z","shell.execute_reply":"2025-04-16T15:37:46.994576Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****Data Analysis","metadata":{}},{"cell_type":"code","source":"\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n\nDATASET_PATH = '/kaggle/input/train-fish/Freshwater Fish Disease Aquaculture in south asia/Train'\n\nclass FishDiseaseDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n        \n        self.samples = []\n        for class_name in self.classes:\n            class_dir = os.path.join(root_dir, class_name)\n            for img_name in os.listdir(class_dir):\n                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n                    self.samples.append((os.path.join(class_dir, img_name), self.class_to_idx[class_name], class_name))\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        img_path, label, class_name = self.samples[idx]\n        image = Image.open(img_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        return {\n            'image': image,\n            'label': label,\n            'path': img_path,\n            'class_name': class_name\n        }\n\n# analyze image properties\ndef analyze_image_properties(dataset):\n    image_sizes = []\n    aspect_ratios = []\n    channels = []\n    file_sizes = []\n    \n    for i in tqdm(range(len(dataset)), desc=\"Analyzing images\"):\n        sample = dataset[i]\n        img_path = sample['path']\n        \n        \n        file_size = os.path.getsize(img_path) / 1024\n        file_sizes.append(file_size)\n        \n        \n        with Image.open(img_path) as img:\n            width, height = img.size\n            image_sizes.append((width, height))\n            aspect_ratios.append(width / height)\n            channels.append(len(img.getbands()))\n    \n    return {\n        'image_sizes': image_sizes,\n        'aspect_ratios': aspect_ratios,\n        'channels': channels,\n        'file_sizes': file_sizes\n    }\n\n# compute pixel statistics\ndef compute_pixel_statistics(dataset, num_samples=1000):\n    # Use a subset of images for pixel statistics to save time\n    indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n    \n    \n    to_tensor = transforms.ToTensor()\n    \n    \n    all_pixels = []\n    \n    for idx in tqdm(indices, desc=\"Computing pixel statistics\"):\n        sample = dataset[idx]\n        img_path = sample['path']\n        \n        \n        with Image.open(img_path) as img:\n            img_tensor = to_tensor(img)\n            all_pixels.append(img_tensor.view(3, -1))\n    \n\n    all_pixels = torch.cat(all_pixels, dim=1)\n    \n    \n    mean = all_pixels.mean(dim=1)\n    std = all_pixels.std(dim=1)\n    \n    return {\n        'mean': mean.tolist(),\n        'std': std.tolist()\n    }\n\n#  visualize class distribution\ndef visualize_class_distribution(dataset):\n    class_counts = Counter([sample[1] for sample in dataset.samples])\n    class_names = [dataset.classes[idx] for idx in sorted(class_counts.keys())]\n    counts = [class_counts[idx] for idx in sorted(class_counts.keys())]\n    \n    plt.figure(figsize=(12, 6))\n    bars = plt.bar(class_names, counts, color='skyblue')\n    plt.title('Class Distribution in Fish Disease Dataset', fontsize=15)\n    plt.xlabel('Disease Classes', fontsize=12)\n    plt.ylabel('Number of Images', fontsize=12)\n    plt.xticks(rotation=45, ha='right')\n    \n    \n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height + 5,\n                 f'{height}', ha='center', va='bottom')\n    \n    plt.tight_layout()\n    plt.savefig('class_distribution.png')\n    plt.show()\n    \n    \n    return pd.DataFrame({\n        'Class': class_names,\n        'Count': counts,\n        'Percentage': [count/sum(counts)*100 for count in counts]\n    })\n\n# visualize sample images from each class\ndef visualize_sample_images(dataset, samples_per_class=5):\n    # Group samples by class\n    class_samples = {}\n    for path, label, class_name in dataset.samples:\n        if class_name not in class_samples:\n            class_samples[class_name] = []\n        class_samples[class_name].append(path)\n    \n    \n    selected_samples = {}\n    for class_name, paths in class_samples.items():\n        selected_samples[class_name] = random.sample(paths, min(samples_per_class, len(paths)))\n    \n    \n    num_classes = len(dataset.classes)\n    fig, axes = plt.subplots(num_classes, samples_per_class, figsize=(15, 2.5*num_classes))\n    \n    for i, class_name in enumerate(dataset.classes):\n        for j, img_path in enumerate(selected_samples[class_name]):\n            img = Image.open(img_path).convert('RGB')\n            axes[i, j].imshow(img)\n            axes[i, j].axis('off')\n            \n            \n            if j == 0:\n                axes[i, j].set_title(f\"{class_name}\", fontsize=10)\n    \n    plt.tight_layout()\n    plt.savefig('sample_images.png')\n    plt.show()\n\n# analyze image dimensions\ndef analyze_image_dimensions(image_sizes):\n    widths, heights = zip(*image_sizes)\n    \n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.hist(widths, bins=20, alpha=0.7, color='blue')\n    plt.title('Distribution of Image Widths')\n    plt.xlabel('Width (pixels)')\n    plt.ylabel('Frequency')\n    \n    plt.subplot(1, 2, 2)\n    plt.hist(heights, bins=20, alpha=0.7, color='green')\n    plt.title('Distribution of Image Heights')\n    plt.xlabel('Height (pixels)')\n    plt.ylabel('Frequency')\n    \n    plt.tight_layout()\n    plt.savefig('image_dimensions.png')\n    plt.show()\n    \n    \n    plt.figure(figsize=(10, 8))\n    plt.scatter(widths, heights, alpha=0.5)\n    plt.title('Image Dimensions: Width vs Height')\n    plt.xlabel('Width (pixels)')\n    plt.ylabel('Height (pixels)')\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.savefig('width_vs_height.png')\n    plt.show()\n    \n    \n    return pd.DataFrame({\n        'Statistic': ['Min', 'Max', 'Mean', 'Median', 'Std Dev'],\n        'Width': [min(widths), max(widths), np.mean(widths), np.median(widths), np.std(widths)],\n        'Height': [min(heights), max(heights), np.mean(heights), np.median(heights), np.std(heights)]\n    })\n\n# analyze aspect ratios\ndef analyze_aspect_ratios(aspect_ratios):\n    plt.figure(figsize=(10, 6))\n    plt.hist(aspect_ratios, bins=30, alpha=0.7, color='purple')\n    plt.title('Distribution of Image Aspect Ratios (Width/Height)')\n    plt.xlabel('Aspect Ratio')\n    plt.ylabel('Frequency')\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.savefig('aspect_ratios.png')\n    plt.show()\n    \n    \n    return pd.DataFrame({\n        'Statistic': ['Min', 'Max', 'Mean', 'Median', 'Std Dev'],\n        'Aspect Ratio': [\n            min(aspect_ratios),\n            max(aspect_ratios),\n            np.mean(aspect_ratios),\n            np.median(aspect_ratios),\n            np.std(aspect_ratios)\n        ]\n    })\n\n# analyze file sizes\ndef analyze_file_sizes(file_sizes):\n    plt.figure(figsize=(10, 6))\n    plt.hist(file_sizes, bins=30, alpha=0.7, color='orange')\n    plt.title('Distribution of Image File Sizes')\n    plt.xlabel('File Size (KB)')\n    plt.ylabel('Frequency')\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.savefig('file_sizes.png')\n    plt.show()\n    \n    \n    return pd.DataFrame({\n        'Statistic': ['Min', 'Max', 'Mean', 'Median', 'Std Dev', 'Total (MB)'],\n        'File Size': [\n            min(file_sizes),\n            max(file_sizes),\n            np.mean(file_sizes),\n            np.median(file_sizes),\n            np.std(file_sizes),\n            sum(file_sizes) / 1024  # Total in MB\n        ]\n    })\n\n# analyze color distributions\ndef analyze_color_distributions(dataset, num_samples=100):\n    # Use a subset of images for color analysis\n    indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n    \n    \n    class_color_histograms = {class_name: [] for class_name in dataset.classes}\n    \n    for idx in tqdm(indices, desc=\"Analyzing color distributions\"):\n        sample = dataset[idx]\n        img_path = sample['path']\n        class_name = sample['class_name']\n        \n        \n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        \n        color_hist = []\n        for i, color in enumerate(['r', 'g', 'b']):\n            hist = cv2.calcHist([img], [i], None, [256], [0, 256])\n            hist = hist / hist.sum()  # Normalize\n            color_hist.append(hist)\n        \n        class_color_histograms[class_name].append(color_hist)\n    \n    \n    plt.figure(figsize=(15, 10))\n    \n    for i, class_name in enumerate(dataset.classes):\n        if not class_color_histograms[class_name]:\n            continue\n            \n        plt.subplot(3, 3, i+1)\n        \n        \n        avg_hist = np.zeros((3, 256))\n        for hist in class_color_histograms[class_name]:\n            for channel in range(3):\n                avg_hist[channel] += hist[channel].flatten()\n        \n        if len(class_color_histograms[class_name]) > 0:\n            avg_hist /= len(class_color_histograms[class_name])\n        \n        # Plot\n        colors = ['r', 'g', 'b']\n        for channel in range(3):\n            plt.plot(avg_hist[channel], color=colors[channel], alpha=0.7)\n        \n        plt.title(f'Color Distribution: {class_name}')\n        plt.xlabel('Pixel Intensity')\n        plt.ylabel('Frequency')\n        plt.xlim([0, 256])\n    \n    plt.tight_layout()\n    plt.savefig('color_distributions.png')\n    plt.show()\n\n#  create a zip file with analyzed images\ndef create_analysis_zip(dataset, output_zip='analyzed_images.zip', samples_per_class=10):\n    # Create a temporary directory to store selected images\n    temp_dir = 'temp_images_for_zip'\n    os.makedirs(temp_dir, exist_ok=True)\n    \n    \n    class_samples = {}\n    for path, label, class_name in dataset.samples:\n        if class_name not in class_samples:\n            class_samples[class_name] = []\n        class_samples[class_name].append(path)\n    \n    \n    selected_samples = {}\n    for class_name, paths in class_samples.items():\n        selected_samples[class_name] = random.sample(paths, min(samples_per_class, len(paths)))\n    \n    \n    for class_name, paths in selected_samples.items():\n        class_dir = os.path.join(temp_dir, class_name)\n        os.makedirs(class_dir, exist_ok=True)\n        \n        for i, img_path in enumerate(paths):\n            img_name = f\"{i+1}_{os.path.basename(img_path)}\"\n            shutil.copy(img_path, os.path.join(class_dir, img_name))\n    \n    \n    with zipfile.ZipFile(output_zip, 'w') as zipf:\n        for root, dirs, files in os.walk(temp_dir):\n            for file in files:\n                file_path = os.path.join(root, file)\n                arcname = os.path.relpath(file_path, temp_dir)\n                zipf.write(file_path, arcname)\n    \n    \n    shutil.rmtree(temp_dir)\n    \n    print(f\"Created zip file '{output_zip}' with sample images\")\n    return output_zip\n\n# Main \ndef analyze_fish_disease_dataset():\n    print(\"Starting Fish Disease Dataset Analysis...\")\n    \n    \n    if not os.path.exists(DATASET_PATH):\n        print(f\"Dataset path {DATASET_PATH} not found. Please update the path.\")\n        return\n    \n    \n    dataset = FishDiseaseDataset(DATASET_PATH)\n    print(f\"Found {len(dataset)} images across {len(dataset.classes)} classes\")\n    print(f\"Classes: {dataset.classes}\")\n    \n    # 1. Class Distribution Analysis\n    print(\"\\n1. Analyzing class distribution...\")\n    class_df = visualize_class_distribution(dataset)\n    print(class_df)\n    \n    # 2. Sample Images Visualization\n    print(\"\\n2. Visualizing sample images from each class...\")\n    visualize_sample_images(dataset)\n    \n    # 3. Image Properties Analysis\n    print(\"\\n3. Analyzing image properties...\")\n    image_props = analyze_image_properties(dataset)\n    \n    # 4. Image Dimensions Analysis\n    print(\"\\n4. Analyzing image dimensions...\")\n    dim_stats = analyze_image_dimensions(image_props['image_sizes'])\n    print(dim_stats)\n    \n    # 5. Aspect Ratio Analysis\n    print(\"\\n5. Analyzing aspect ratios...\")\n    ar_stats = analyze_aspect_ratios(image_props['aspect_ratios'])\n    print(ar_stats)\n    \n    # 6. File Size Analysis\n    print(\"\\n6. Analyzing file sizes...\")\n    size_stats = analyze_file_sizes(image_props['file_sizes'])\n    print(size_stats)\n    \n    # 7. Color Distribution Analysis\n    print(\"\\n7. Analyzing color distributions...\")\n    analyze_color_distributions(dataset)\n    \n    # 8. Pixel Statistics\n    print(\"\\n8. Computing pixel statistics...\")\n    pixel_stats = compute_pixel_statistics(dataset)\n    print(f\"Channel means: {pixel_stats['mean']}\")\n    print(f\"Channel standard deviations: {pixel_stats['std']}\")\n    \n    # 9. Create zip file with analyzed images\n    print(\"\\n9. Creating zip file with sample images...\")\n    zip_path = create_analysis_zip(dataset)\n    \n    print(\"\\nAnalysis complete! All visualizations have been saved as PNG files.\")\n    print(f\"Sample images have been saved to {zip_path}\")\n    \n    # Return a summary of findings\n    return {\n        'num_images': len(dataset),\n        'classes': dataset.classes,\n        'class_distribution': class_df,\n        'dimension_stats': dim_stats,\n        'aspect_ratio_stats': ar_stats,\n        'file_size_stats': size_stats,\n        'pixel_stats': pixel_stats\n    }\n\n\nif __name__ == \"__main__\":\n    results = analyze_fish_disease_dataset()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****Preprocessing","metadata":{}},{"cell_type":"code","source":"\ndef preprocess_dataset(input_dir, output_dir, target_size=(224, 224)):\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    \n    \n    class_folders = [f for f in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, f))]\n    print(f\"Found {len(class_folders)} classes: {class_folders}\")\n    \n    \n    total_images = 0\n    corrupted_images = 0\n    \n    \n    for class_name in class_folders:\n        class_path = os.path.join(input_dir, class_name)\n        output_class_path = os.path.join(output_dir, class_name)\n        \n        \n        os.makedirs(output_class_path, exist_ok=True)\n        \n        \n        image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n        print(f\"Processing {len(image_files)} images in class '{class_name}'\")\n        \n        \n        for img_file in tqdm(image_files, desc=f\"Class {class_name}\"):\n            total_images += 1\n            img_path = os.path.join(class_path, img_file)\n            output_img_path = os.path.join(output_class_path, img_file)\n            \n            try:\n                \n                with Image.open(img_path) as img:\n                    \n                    resized_img = img.resize(target_size, Image.LANCZOS)\n                    \n                    \n                    resized_img.save(output_img_path)\n            except Exception as e:\n                print(f\"Corrupted or invalid image: {img_path}\")\n                print(f\"Error: {str(e)}\")\n                corrupted_images += 1\n    \n    print(\"\\nPreprocessing completed!\")\n    print(f\"Total images processed: {total_images}\")\n    print(f\"Corrupted images found: {corrupted_images}\")\n    print(f\"Preprocessed dataset saved to: {output_dir}\")\n\n\nif __name__ == \"__main__\":\n    \n    input_dataset_path = \"/kaggle/input/train-fish/Freshwater Fish Disease Aquaculture in south asia/Train\"\n    output_dataset_path = \"/kaggle/working/preprocessed_fish_disease_dataset\"\n    \n    \n    preprocess_dataset(input_dataset_path, output_dataset_path, target_size=(224, 224))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T15:38:08.946241Z","iopub.execute_input":"2025-04-16T15:38:08.946972Z","iopub.status.idle":"2025-04-16T15:38:28.895881Z","shell.execute_reply.started":"2025-04-16T15:38:08.946948Z","shell.execute_reply":"2025-04-16T15:38:28.895083Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****Data Split","metadata":{}},{"cell_type":"code","source":"\nrandom.seed(42)\n\n\noriginal_data_dir = '/kaggle/working/preprocessed_fish_disease_dataset'\nbase_output_dir = '/kaggle/working/'\n\n\nsplits = ['train', 'val', 'test']\nfor split in splits:\n    for class_name in os.listdir(original_data_dir):\n        split_path = os.path.join(base_output_dir, split, class_name)\n        os.makedirs(split_path, exist_ok=True)\n\n\nfor class_name in os.listdir(original_data_dir):\n    class_path = os.path.join(original_data_dir, class_name)\n    images = os.listdir(class_path)\n    random.shuffle(images)\n    \n    total_images = len(images)\n    train_end = int(0.8 * total_images)\n    val_end = int(0.9 * total_images)\n    \n    train_images = images[:train_end]\n    val_images = images[train_end:val_end]\n    test_images = images[val_end:]\n    \n    for image in train_images:\n        src = os.path.join(class_path, image)\n        dst = os.path.join(base_output_dir, 'train', class_name, image)\n        shutil.copy2(src, dst)\n        \n    for image in val_images:\n        src = os.path.join(class_path, image)\n        dst = os.path.join(base_output_dir, 'val', class_name, image)\n        shutil.copy2(src, dst)\n        \n    for image in test_images:\n        src = os.path.join(class_path, image)\n        dst = os.path.join(base_output_dir, 'test', class_name, image)\n        shutil.copy2(src, dst)\n\nprint(\"Dataset split completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T15:38:40.583709Z","iopub.execute_input":"2025-04-16T15:38:40.584044Z","iopub.status.idle":"2025-04-16T15:38:40.793672Z","shell.execute_reply.started":"2025-04-16T15:38:40.584015Z","shell.execute_reply":"2025-04-16T15:38:40.792792Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****Augmenting","metadata":{}},{"cell_type":"code","source":"\ntrain_dir = '/kaggle/working/train'\n\n\nbefore_counts = {}\nfor class_folder in os.listdir(train_dir):\n    class_path = os.path.join(train_dir, class_folder)\n    if os.path.isdir(class_path):\n        count = len(os.listdir(class_path))\n        before_counts[class_folder] = count\n\nprint(\"Image count BEFORE augmentation:\")\nfor class_name, count in before_counts.items():\n    print(f\"{class_name}: {count} images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T15:38:44.645161Z","iopub.execute_input":"2025-04-16T15:38:44.645479Z","iopub.status.idle":"2025-04-16T15:38:44.652826Z","shell.execute_reply.started":"2025-04-16T15:38:44.645456Z","shell.execute_reply":"2025-04-16T15:38:44.652047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\naugment_transform = transforms.Compose([\n    transforms.RandomRotation(20),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=(0.7, 1.3)),\n    transforms.RandomAffine(degrees=0, shear=10),\n    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n    transforms.ToTensor()\n])\n\n \ntrain_dir = '/kaggle/working/train'\n\n\naugmentations_per_image = 1  \n\n\nfor class_folder in os.listdir(train_dir):\n    class_path = os.path.join(train_dir, class_folder)\n    if os.path.isdir(class_path):\n        for img_name in os.listdir(class_path):\n            img_path = os.path.join(class_path, img_name)\n            try:\n                img = Image.open(img_path).convert('RGB')  # Load image safely\n\n                for i in range(augmentations_per_image):\n                    augmented = augment_transform(img)\n                    save_path = os.path.join(class_path, f\"aug_{random.randint(10000,99999)}.jpg\")\n                    save_image(augmented, save_path)\n            \n            except Exception as e:\n                print(f\"Error processing {img_path}: {e}\")","metadata":{"execution":{"iopub.status.busy":"2025-04-16T15:39:01.372840Z","iopub.execute_input":"2025-04-16T15:39:01.373111Z","iopub.status.idle":"2025-04-16T15:39:16.779655Z","shell.execute_reply.started":"2025-04-16T15:39:01.373088Z","shell.execute_reply":"2025-04-16T15:39:16.779001Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrain_dir = '/kaggle/working/train'\n\n\nbefore_counts = {}\nfor class_folder in os.listdir(train_dir):\n    class_path = os.path.join(train_dir, class_folder)\n    if os.path.isdir(class_path):\n        count = len(os.listdir(class_path))\n        before_counts[class_folder] = count\n\nprint(\"Image count AFTER augmentation:\")\nfor class_name, count in before_counts.items():\n    print(f\"{class_name}: {count} images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T15:39:24.384624Z","iopub.execute_input":"2025-04-16T15:39:24.385172Z","iopub.status.idle":"2025-04-16T15:39:24.394311Z","shell.execute_reply.started":"2025-04-16T15:39:24.385145Z","shell.execute_reply":"2025-04-16T15:39:24.393609Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****EfficientNet","metadata":{}},{"cell_type":"code","source":"\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\nTRAIN_PATH = \"/kaggle/working/train\"\nVAL_PATH = \"/kaggle/working/val\"\nTEST_PATH = \"/kaggle/working/test\"\nOUTPUT_DIR = \"/kaggle/working/\"\n\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\naugment_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet normalization\n])\n\n\nval_test_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet normalization\n])\n\n\ntrain_dataset = ImageFolder(TRAIN_PATH, transform=augment_transform)\nval_dataset = ImageFolder(VAL_PATH, transform=val_test_transform)\ntest_dataset = ImageFolder(TEST_PATH, transform=val_test_transform)\n\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\n\nclass_names = train_dataset.classes\nnum_classes = len(class_names)\nprint(f\"Classes: {class_names}\")\nprint(f\"Number of classes: {num_classes}\")\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Validation samples: {len(val_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")\n\n\ndef load_model(model_name='efficientnet_b0', num_classes=7):\n    if model_name == 'efficientnet_b0':\n        model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n    elif model_name == 'efficientnet_b1':\n        model = models.efficientnet_b1(weights='IMAGENET1K_V1')\n    elif model_name == 'efficientnet_b2':\n        model = models.efficientnet_b2(weights='IMAGENET1K_V1')\n    elif model_name == 'efficientnet_b3':\n        model = models.efficientnet_b3(weights='IMAGENET1K_V1')\n    else:\n        model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n    \n    \n    for param in list(model.parameters())[:-20]:  # Freeze all but the last few layers\n        param.requires_grad = False\n    \n    \n    in_features = model.classifier[1].in_features\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.2, inplace=True),\n        nn.Linear(in_features=in_features, out_features=num_classes)\n    )\n    \n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f'Trainable parameters: {trainable_params:,}')\n    return model\n\n# train the model\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25, early_stopping_patience=10):\n    since = time.time()\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    \n    no_improve_epochs = 0\n    \n    \n    train_losses = []\n    val_losses = []\n    train_accs = []\n    val_accs = []\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print('-' * 10)\n        \n        \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  \n                dataloader = train_loader\n            else:\n                model.eval()   \n                dataloader = val_loader\n            \n            running_loss = 0.0\n            running_corrects = 0\n            \n            \n            for inputs, labels in tqdm(dataloader, desc=phase):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                \n                optimizer.zero_grad()\n                \n                # Forward pass\n                \n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    \n                    # Backward + optimize only in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                \n                \n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            \n            epoch_loss = running_loss / len(dataloader.dataset)\n            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n            \n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n            \n            \n            if phase == 'train':\n                train_losses.append(epoch_loss)\n                train_accs.append(epoch_acc.item())\n            else:\n                val_losses.append(epoch_loss)\n                val_accs.append(epoch_acc.item())\n                \n                \n                scheduler.step(epoch_loss)\n                \n                \n                if epoch_acc > best_acc:\n                    best_acc = epoch_acc\n                    best_model_wts = copy.deepcopy(model.state_dict())\n                    no_improve_epochs = 0\n                else:\n                    no_improve_epochs += 1\n        \n        \n        if no_improve_epochs >= early_stopping_patience:\n            print(f'Early stopping triggered after {epoch+1} epochs')\n            break\n            \n        print()\n    \n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val Acc: {best_acc:.4f}')\n    \n    \n    model.load_state_dict(best_model_wts)\n    \n    \n    return model, {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'train_accs': train_accs,\n        'val_accs': val_accs\n    }\n\n# evaluate model on test set\ndef evaluate_model(model, dataloader):\n    model.eval()\n    \n    all_preds = []\n    all_labels = []\n    all_probs = []\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(dataloader, desc='Testing'):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(inputs)\n            probs = torch.nn.functional.softmax(outputs, dim=1)\n            _, preds = torch.max(outputs, 1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n    \n    \n    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n    print(f'Test Accuracy: {accuracy:.4f}')\n    \n    \n    print(\"\\nClassification Report:\")\n    print(classification_report(all_labels, all_preds, target_names=class_names))\n    \n    return all_preds, all_labels, all_probs\n\n\ndef plot_confusion_matrix(y_true, y_pred, classes):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n    plt.show()\n\n\ndef plot_learning_curves(history):\n    plt.figure(figsize=(12, 5))\n    \n    \n    plt.subplot(1, 2, 1)\n    plt.plot(history['train_losses'], label='Training Loss')\n    plt.plot(history['val_losses'], label='Validation Loss')\n    plt.title('Loss Curves')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True, linestyle='--', alpha=0.6)\n    \n    \n    plt.subplot(1, 2, 2)\n    plt.plot(history['train_accs'], label='Training Accuracy')\n    plt.plot(history['val_accs'], label='Validation Accuracy')\n    plt.title('Accuracy Curves')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True, linestyle='--', alpha=0.6)\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'learning_curves.png'), dpi=300, bbox_inches='tight')\n    plt.show()\n\n\ndef plot_roc_curves(y_true, y_probs, classes):\n    \n    y_true_bin = label_binarize(y_true, classes=range(len(classes)))\n    \n    \n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    \n    for i in range(len(classes)):\n        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_probs[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n    \n    \n    plt.figure(figsize=(12, 10))\n    \n    for i in range(len(classes)):\n        plt.plot(fpr[i], tpr[i], lw=2, label=f'{classes[i]} (AUC = {roc_auc[i]:.2f})')\n    \n    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curves for Each Class')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True, linestyle='--', alpha=0.6)\n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'roc_curves.png'), dpi=300, bbox_inches='tight')\n    plt.show()\n\n\ndef check_and_fix_fitting(history):\n    train_losses = history['train_losses']\n    val_losses = history['val_losses']\n    train_accs = history['train_accs']\n    val_accs = history['val_accs']\n    \n    \n    loss_gap = np.array(train_losses) - np.array(val_losses)\n    acc_gap = np.array(train_accs) - np.array(val_accs)\n    \n    \n    if np.mean(loss_gap[-5:]) < -0.1 and np.mean(acc_gap[-5:]) > 0.1:\n        print(\"Detected OVERFITTING: Training accuracy is much higher than validation accuracy.\")\n        return \"overfitting\"\n    \n    \n    elif np.mean(train_accs[-5:]) < 0.7 and np.mean(val_accs[-5:]) < 0.7:\n        print(\"Detected UNDERFITTING: Both training and validation accuracy are low.\")\n        return \"underfitting\"\n    \n    \n    else:\n        print(\"Model appears to be well-fitted: No significant overfitting or underfitting detected.\")\n        return \"good_fit\"\n\n\ndef apply_fitting_strategies(diagnosis, model, optimizer):\n    if diagnosis == \"overfitting\":\n        print(\"Applying strategies to reduce overfitting...\")\n        \n        #  Increase dropout\n        for module in model.modules():\n            if isinstance(module, nn.Dropout):\n                module.p = min(module.p + 0.1, 0.5)  # Increase dropout but cap at 0.5\n                print(f\"Increased dropout to {module.p}\")\n        \n        # Apply weight decay\n        for param_group in optimizer.param_groups:\n            param_group['weight_decay'] = 1e-4\n            print(f\"Added weight decay: {param_group['weight_decay']}\")\n        \n        return model\n        \n    elif diagnosis == \"underfitting\":\n        print(\"Applying strategies to reduce underfitting...\")\n        \n        # Decrease dropout\n        for module in model.modules():\n            if isinstance(module, nn.Dropout):\n                module.p = max(module.p - 0.1, 0.1)  # Decrease dropout but keep at least 0.1\n                print(f\"Decreased dropout to {module.p}\")\n        \n        # Unfreeze more layers for fine-tuning\n        ct = 0\n        for child in model.children():\n            ct += 1\n            if ct > 6:  # Unfreeze more layers\n                for param in child.parameters():\n                    param.requires_grad = True\n        \n        print(\"Unfrozen more layers for fine-tuning\")\n        \n        # Increase learning rate\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = param_group['lr'] * 2\n            print(f\"Increased learning rate to {param_group['lr']}\")\n        \n        return model\n    \n    else:  \n        print(\"Model is well-fitted. No changes needed.\")\n        return model\n\n\ndef main():\n    \n    model = load_model(model_name='efficientnet_b0', num_classes=num_classes)\n    model = model.to(device)\n    \n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    \n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n    \n    print(\"Starting training...\")\n    \n    model, history = train_model(\n        model, \n        criterion, \n        optimizer, \n        scheduler,\n        num_epochs=20,\n        early_stopping_patience=7\n    )\n    \n    \n    plot_learning_curves(history)\n    \n    \n    diagnosis = check_and_fix_fitting(history)\n    \n    \n    if diagnosis != \"good_fit\":\n        print(\"\\nApplying strategies and retraining...\")\n        model = apply_fitting_strategies(diagnosis, model, optimizer)\n        \n        \n        optimizer = optim.Adam(model.parameters(), lr=0.0005)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n        \n        \n        model, history2 = train_model(\n            model, \n            criterion, \n            optimizer, \n            scheduler,\n            num_epochs=15,\n            early_stopping_patience=5\n        )\n        \n        \n        plot_learning_curves(history2)\n    \n    \n    torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, 'efficientnet_fish_disease.pth'))\n    print(f\"Model saved to {os.path.join(OUTPUT_DIR, 'efficientnet_fish_disease.pth')}\")\n    \n    \n    print(\"\\nEvaluating on test set...\")\n    y_pred, y_true, y_probs = evaluate_model(model, test_loader)\n    \n    \n    plot_confusion_matrix(y_true, y_pred, class_names)\n    \n    \n    plot_roc_curves(y_true, y_probs, class_names)\n    \n    print(\"\\nAll plots have been saved to the output directory.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T15:41:19.339496Z","iopub.execute_input":"2025-04-16T15:41:19.339840Z","iopub.status.idle":"2025-04-16T15:44:16.266866Z","shell.execute_reply.started":"2025-04-16T15:41:19.339805Z","shell.execute_reply":"2025-04-16T15:44:16.265856Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****DenseNet","metadata":{}},{"cell_type":"code","source":"\nimagenet_mean = [0.485, 0.456, 0.406]\nimagenet_std = [0.229, 0.224, 0.225]\naugment_transform = transforms.Compose([\n    \n    transforms.ToTensor(),\n    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n    \n])\n\n\n\ndata_transform = {\n    'train': augment_transform,\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n    ])\n}\n\n\ndata_dir = \"/kaggle/working/\"\ndatasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), transform=data_transform[x]) for x in ['train', 'val', 'test']}\nloaders = {x: DataLoader(datasets[x], batch_size=32, shuffle=True) for x in ['train', 'val', 'test']}\nclass_names = datasets['train'].classes\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the pretrained DenseNet model\nmodel = models.densenet121(pretrained=True)\n\n# Replace the classifier for the current task\nnum_features = model.classifier.in_features\nmodel.classifier = nn.Linear(num_features, len(class_names))\nmodel = model.to(device)\n\n\ndef train_and_validate(model, loaders, criterion, optimizer, num_epochs=10):\n    train_loss, val_loss = [], []\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        \n        # Training phase\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in loaders['train']:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        train_loss.append(running_loss / len(loaders['train']))\n\n        # Validation phase\n        model.eval()\n        val_running_loss = 0.0\n        with torch.no_grad():\n            for inputs, labels in loaders['val']:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_running_loss += loss.item()\n        val_loss.append(val_running_loss / len(loaders['val']))\n\n        print(f\"Train Loss: {train_loss[-1]:.4f}, Val Loss: {val_loss[-1]:.4f}\")\n    return train_loss, val_loss\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ntrain_loss, val_loss = train_and_validate(model, loaders, criterion, optimizer,num_epochs=20)\n\nplt.figure()\nplt.plot(train_loss, label=\"Train Loss\")  # Training Loss\nplt.plot(val_loss, label=\"Validation Loss\")  # Validation Loss\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Learning Curve\")\nplt.savefig(\"learning_curve_densenet.png\")  # Save the learning curve image\nplt.show()\n\n\nif val_loss[-1] > train_loss[-1] and (val_loss[-1] - train_loss[-1]) > 0.05:\n    print(\"The DenseNet model is overfitting.\")\nelif val_loss[-1] < train_loss[-1] and (train_loss[-1] - val_loss[-1]) > 0.05:\n    print(\"The DenseNet model is underfitting.\")\nelse:\n    print(\"The DenseNet model is fitting well.\")\n\n\nplt.figure()\nplt.plot(train_loss, label=\"Train Loss\")\nplt.plot(val_loss, label=\"Validation Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Learning Curve\")\nplt.savefig(\"learning_curve.png\")\nplt.show()\n\n\ndef test_model(model, loader):\n    model.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    return all_labels, all_preds\n\nlabels, preds = test_model(model, loaders['test'])\n\n\nreport = classification_report(labels, preds, target_names=class_names)\nprint(\"Classification Report:\")\nprint(report)\n\n\ntest_accuracy = accuracy_score(labels, preds)\nprint(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n\n\ncm = confusion_matrix(labels, preds)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.savefig(\"confusion_matrix.png\")\nplt.show()\n\n\n\ndef evaluate_model_probs(model, loader):\n    model.eval()\n    all_probs, all_labels = [], []\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            probabilities = torch.softmax(outputs, dim=1)  # Get probabilities for all classes\n            all_probs.extend(probabilities.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    return np.array(all_labels), np.array(all_probs)\n\n\nlabels, outputs_probs = evaluate_model_probs(model, loaders['test'])\n\n\nfrom sklearn.preprocessing import label_binarize\nlabels_one_hot = label_binarize(labels, classes=range(len(class_names)))\n\n\nfpr, tpr, roc_auc = {}, {}, {}\nfor i in range(len(class_names)):\n    fpr[i], tpr[i], _ = roc_curve(labels_one_hot[:, i], outputs_probs[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n\nplt.figure(figsize=(10, 8))\nfor i in range(len(class_names)):\n    plt.plot(fpr[i], tpr[i], label=f\"Class {class_names[i]} (AUC = {roc_auc[i]:.2f})\")\nplt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC AUC Curve\")\nplt.legend(loc=\"lower right\")\nplt.savefig(\"roc_auc_curve.png\")\nplt.show()\n\n\n\n\n   \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****Overfit","metadata":{}},{"cell_type":"code","source":"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nimagenet_mean = [0.485, 0.456, 0.406]\nimagenet_std = [0.229, 0.224, 0.225]\n\naugment_transform = transforms.Compose([\n    transforms.RandomRotation(20),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=(0.7, 1.3)),\n    transforms.RandomAffine(degrees=0, shear=10),\n    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n    transforms.ToTensor(),\n    transforms.RandomErasing(p=0.5),\n    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n])\n\ndata_transform = {\n    'train': augment_transform,\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n    ])\n}\n\n\ndata_dir = \"/kaggle/working/\"  \ndatasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), transform=data_transform[x]) for x in ['train', 'val', 'test']}\nloaders = {x: DataLoader(datasets[x], batch_size=32, shuffle=True) for x in ['train', 'val', 'test']}\nclass_names = datasets['train'].classes\n\n\nmodel = models.densenet121(pretrained=True)\n\n\nfor param in model.features.parameters():\n    param.requires_grad = False  \n\nfor name, param in list(model.features.named_parameters())[-5:]:  # Unfreeze the last 5 layers for fine-tuning\n    param.requires_grad = True\n\nnum_features = model.classifier.in_features\nmodel.classifier = nn.Sequential(\n    nn.Dropout(p=0.3),  # Reduced dropout rate\n    nn.Linear(num_features, len(class_names))\n)\nmodel = model.to(device)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'Trainable parameters: {trainable_params:,}')\n\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)  \n\n\ndef train_and_validate(model, loaders, criterion, optimizer, scheduler, num_epochs=30, patience=5):\n    train_loss, val_loss = [], []\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n\n        \n        model.train()\n        running_loss = 0.0\n        for inputs, labels in loaders['train']:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        train_loss.append(running_loss / len(loaders['train']))\n\n        \n        model.eval()\n        val_running_loss = 0.0\n        with torch.no_grad():\n            for inputs, labels in loaders['val']:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_running_loss += loss.item()\n        val_loss.append(val_running_loss / len(loaders['val']))\n\n        print(f\"Train Loss: {train_loss[-1]:.4f}, Val Loss: {val_loss[-1]:.4f}\")\n\n        \n        scheduler.step()\n\n        \n        if val_loss[-1] < best_val_loss:\n            best_val_loss = val_loss[-1]\n            patience_counter = 0\n            torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n    \n    print(\"\\nFit Assessment:\")\n    if val_loss[-1] > train_loss[-1] and (val_loss[-1] - train_loss[-1]) > 0.05:\n        print(\"The DenseNet model is overfitting.\")\n    elif val_loss[-1] < train_loss[-1] and (train_loss[-1] - val_loss[-1]) > 0.05:\n        print(\"The DenseNet model is underfitting.\")\n    else:\n        print(\"The DenseNet model is fitting well.\")\n\n    return train_loss, val_loss\n\n\ntrain_loss, val_loss = train_and_validate(model, loaders, criterion, optimizer, scheduler)\n\n\nplt.figure()\nplt.plot(train_loss, label=\"Train Loss\")\nplt.plot(val_loss, label=\"Validation Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Learning Curve\")\nplt.savefig(\"learning_curve_densenet.png\")\nplt.show()\n\n\ndef evaluate_model(model, loader):\n    model.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    return all_labels, all_preds\n\nlabels, preds = evaluate_model(model, loaders['test'])\n\n\nreport = classification_report(labels, preds, target_names=class_names)\nprint(\"\\nClassification Report:\")\nprint(report)\n\n\ntest_accuracy = accuracy_score(labels, preds)\nprint(f\"\\nTest Accuracy: {test_accuracy * 100:.2f}%\")\n\n\ncm = confusion_matrix(labels, preds)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.savefig(\"confusion_matrix_densenet.png\")\nplt.show()\n\n\ndef evaluate_model_probs(model, loader):\n    model.eval()\n    all_probs, all_labels = [], []\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            probabilities = torch.softmax(outputs, dim=1)\n            all_probs.extend(probabilities.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    return np.array(all_labels), np.array(all_probs)\n\nlabels, outputs_probs = evaluate_model_probs(model, loaders['test'])\nlabels_one_hot = label_binarize(labels, classes=range(len(class_names)))\n\nfpr, tpr, roc_auc = {}, {}, {}\nfor i in range(len(class_names)):\n    fpr[i], tpr[i], _ = roc_curve(labels_one_hot[:, i], outputs_probs[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nplt.figure(figsize=(10, 8))\nfor i in range(len(class_names)):\n    plt.plot(fpr[i], tpr[i], label=f\"Class {class_names[i]} (AUC = {roc_auc[i]:.2f})\")\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC AUC Curve\")\nplt.legend(loc=\"lower right\")\nplt.savefig(\"roc_auc_curve_densenet.png\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T15:48:15.326677Z","iopub.execute_input":"2025-04-16T15:48:15.326971Z","iopub.status.idle":"2025-04-16T15:49:03.488982Z","shell.execute_reply.started":"2025-04-16T15:48:15.326949Z","shell.execute_reply":"2025-04-16T15:49:03.487838Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****ResNet50","metadata":{}},{"cell_type":"code","source":"\nimagenet_mean = [0.485, 0.456, 0.406]\nimagenet_std = [0.229, 0.224, 0.225]\n\naugment_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n])\n\ndata_transform = {\n    'train': augment_transform,\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n    ])\n}\n\n\ndata_dir = \"/kaggle/working/\"  \ndatasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), transform=data_transform[x]) for x in ['train', 'val', 'test']}\nloaders = {x: DataLoader(datasets[x], batch_size=32, shuffle=True) for x in ['train', 'val', 'test']}\nclass_names = datasets['train'].classes\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load pretrained ResNet50 model\nmodel = models.resnet50(pretrained=True)\n\n# Replace the final layer to fit the dataset\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, len(class_names))\nmodel = model.to(device)\n\ndef train_and_validate(model, loaders, criterion, optimizer, num_epochs=20):\n    train_loss, val_loss = [], []\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        \n        # Training phase\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in loaders['train']:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        train_loss.append(running_loss / len(loaders['train']))\n\n        # Validation phase\n        model.eval()\n        val_running_loss = 0.0\n        with torch.no_grad():\n            for inputs, labels in loaders['val']:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_running_loss += loss.item()\n        val_loss.append(val_running_loss / len(loaders['val']))\n\n        print(f\"Train Loss: {train_loss[-1]:.4f}, Val Loss: {val_loss[-1]:.4f}\")\n    return train_loss, val_loss\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ntrain_loss, val_loss = train_and_validate(model, loaders, criterion, optimizer)\n\nplt.figure()\nplt.plot(train_loss, label=\"Train Loss\")\nplt.plot(val_loss, label=\"Validation Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Learning Curve\")\nplt.savefig(\"learning_curve.png\")\nplt.show()\n\n\nif val_loss[-1] > train_loss[-1] and val_loss[-1] - train_loss[-1] > 0.05:\n    print(\"The model is overfitting.\")\nelif val_loss[-1] > train_loss[-1]:\n    print(\"The model is slightly overfitting.\")\nelif train_loss[-1] > val_loss[-1]:\n    print(\"The model is underfitting.\")\nelse:\n    print(\"The model is fitting well.\")\n\ndef evaluate_model(model, loader):\n    model.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    return all_labels, all_preds\n\nlabels, preds = evaluate_model(model, loaders['test'])\n\ncm = confusion_matrix(labels, preds)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.savefig(\"confusion_matrix.png\")\nplt.show()\n\n\n\ndef evaluate_model_probs(model, loader):\n    model.eval()\n    all_probs, all_labels = [], []\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            probabilities = torch.softmax(outputs, dim=1)\n            all_probs.extend(probabilities.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    return np.array(all_labels), np.array(all_probs)\n\nlabels, outputs_probs = evaluate_model_probs(model, loaders['test'])\nlabels_one_hot = label_binarize(labels, classes=range(len(class_names)))\n\nfpr, tpr, roc_auc = {}, {}, {}\nfor i in range(len(class_names)):\n    fpr[i], tpr[i], _ = roc_curve(labels_one_hot[:, i], outputs_probs[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nplt.figure(figsize=(10, 8))\nfor i in range(len(class_names)):\n    plt.plot(fpr[i], tpr[i], label=f\"Class {class_names[i]} (AUC = {roc_auc[i]:.2f})\")\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC AUC Curve\")\nplt.legend(loc=\"lower right\")\nplt.savefig(\"roc_auc_curve.png\")\nplt.show()\n\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Overfit","metadata":{}},{"cell_type":"code","source":"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nimagenet_mean = [0.485, 0.456, 0.406]\nimagenet_std = [0.229, 0.224, 0.225]\n\naugment_transform = transforms.Compose([\n    transforms.RandomRotation(20),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=(0.7, 1.3)),\n    transforms.RandomAffine(degrees=0, shear=10),\n    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n    transforms.ToTensor(),\n    transforms.RandomErasing(p=0.5),  # Advanced augmentation\n    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n])\n\ndata_transform = {\n    'train': augment_transform,\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n    ])\n}\n\n\ndata_dir = \"/kaggle/working/\"  \ndatasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), transform=data_transform[x]) for x in ['train', 'val', 'test']}\nloaders = {x: DataLoader(datasets[x], batch_size=32, shuffle=True) for x in ['train', 'val', 'test']}\nclass_names = datasets['train'].classes\n\nfrom torchvision import models\n\n# Replace ResNet50 with ResNet34 for reduced model complexity\nmodel = models.resnet34(pretrained=True)\nnum_features = model.fc.in_features\nmodel.fc = nn.Sequential(\n    nn.Dropout(p=0.5),  \n    nn.Linear(num_features, len(class_names))\n)\nmodel = model.to(device)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'Trainable parameters: {trainable_params:,}')\n\n\n\ndef train_and_validate(model, loaders, criterion, optimizer, num_epochs=20, patience=5):\n    train_loss, val_loss = [], []\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in loaders['train']:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        train_loss.append(running_loss / len(loaders['train']))\n\n        # Validation phase\n        model.eval()\n        val_running_loss = 0.0\n        with torch.no_grad():\n            for inputs, labels in loaders['val']:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_running_loss += loss.item()\n        val_loss.append(val_running_loss / len(loaders['val']))\n\n        print(f\"Train Loss: {train_loss[-1]:.4f}, Val Loss: {val_loss[-1]:.4f}\")\n\n        \n        if val_loss[-1] < best_val_loss:\n            best_val_loss = val_loss[-1]\n            patience_counter = 0\n            torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n    \n    print(\"\\nFit Assessment:\")\n    if val_loss[-1] > train_loss[-1] and (val_loss[-1] - train_loss[-1]) > 0.05:\n        print(\"The ResNet34 model is overfitting.\")\n    elif val_loss[-1] < train_loss[-1] and (train_loss[-1] - val_loss[-1]) > 0.05:\n        print(\"The ResNet34 model is underfitting.\")\n    else:\n        print(\"The ResNet34 model is fitting well.\")\n\n    return train_loss, val_loss\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n\n\ntrain_loss, val_loss = train_and_validate(model, loaders, criterion, optimizer)\n\n\nplt.figure()\nplt.plot(train_loss, label=\"Train Loss\")\nplt.plot(val_loss, label=\"Validation Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Learning Curve\")\nplt.savefig(\"learning_curve.png\")\nplt.show()\n\n\ndef evaluate_model(model, loader):\n    model.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    return all_labels, all_preds\n\nlabels, preds = evaluate_model(model, loaders['test'])\n\n\nreport = classification_report(labels, preds, target_names=class_names)\nprint(\"\\nClassification Report:\")\nprint(report)\n\n\ntest_accuracy = accuracy_score(labels, preds)\nprint(f\"\\nTest Accuracy: {test_accuracy * 100:.2f}%\")\n\n# Confusion matri\ncm = confusion_matrix(labels, preds)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.savefig(\"confusion_matrix.png\")\nplt.show()\n\n\ndef evaluate_model_probs(model, loader):\n    model.eval()\n    all_probs, all_labels = [], []\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            probabilities = torch.softmax(outputs, dim=1)\n            all_probs.extend(probabilities.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    return np.array(all_labels), np.array(all_probs)\n\nlabels, outputs_probs = evaluate_model_probs(model, loaders['test'])\nlabels_one_hot = label_binarize(labels, classes=range(len(class_names)))\n\nfpr, tpr, roc_auc = {}, {}, {}\nfor i in range(len(class_names)):\n    fpr[i], tpr[i], _ = roc_curve(labels_one_hot[:, i], outputs_probs[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nplt.figure(figsize=(10, 8))\nfor i in range(len(class_names)):\n    plt.plot(fpr[i], tpr[i], label=f\"Class {class_names[i]} (AUC = {roc_auc[i]:.2f})\")\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC AUC Curve\")\nplt.legend(loc=\"lower right\")\nplt.savefig(\"roc_auc_curve.png\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T15:52:29.843898Z","iopub.execute_input":"2025-04-16T15:52:29.844659Z","iopub.status.idle":"2025-04-16T15:52:56.026382Z","shell.execute_reply.started":"2025-04-16T15:52:29.844632Z","shell.execute_reply":"2025-04-16T15:52:56.025214Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****AquaScan","metadata":{}},{"cell_type":"code","source":"\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n}\n\n\ndata_dir = '/kaggle/working/'  \nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) \n                 for x in ['train', 'val', 'test']}\ndataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4)\n              for x in ['train', 'val', 'test']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\nclass_names = image_datasets['train'].classes\nnum_classes = len(class_names)\n\nprint(f\"Classes: {class_names}\")\nprint(f\"Number of training images: {dataset_sizes['train']}\")\nprint(f\"Number of validation images: {dataset_sizes['val']}\")\nprint(f\"Number of test images: {dataset_sizes['test']}\")\n\n# Define the AquaScan CNN model\nclass AquaScanCNN(nn.Module):\n    def __init__(self, num_classes=7):\n        super(AquaScanCNN, self).__init__()\n        \n        # Convolutional Block 1\n        self.conv_block1 = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(32),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(0.25)\n        )\n        \n        # Convolutional Block 2\n        self.conv_block2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(0.25)\n        )\n        \n        # Convolutional Block 3\n        self.conv_block3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(0.3)\n        )\n        \n        # Convolutional Block 4\n        self.conv_block4 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(256),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(0.4)\n        )\n        \n        \n        self.fc_input_size = 14 * 14 * 256\n        \n        # Fully Connected Block\n        self.fc_block = nn.Sequential(\n            nn.Linear(self.fc_input_size, 256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5)\n        )\n        \n        # Output Layer\n        self.output_layer = nn.Linear(256, num_classes)\n        \n    def forward(self, x):\n        x = self.conv_block1(x)\n        x = self.conv_block2(x)\n        x = self.conv_block3(x)\n        x = self.conv_block4(x)\n        \n        x = x.view(x.size(0), -1)  # Flatten\n        x = self.fc_block(x)\n        x = self.output_layer(x)\n        \n        return x\n\n\nmodel = AquaScanCNN(num_classes=num_classes).to(device)\nprint(model)\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=50, early_stopping_patience=7):\n    since = time.time()\n    \n   \n    best_model_wts = model.state_dict()\n    best_acc = 0.0\n    \n    \n    train_losses = []\n    val_losses = []\n    train_accs = []\n    val_accs = []\n    \n    \n    best_val_loss = float('inf')\n    early_stopping_counter = 0\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print('-' * 10)\n        \n        \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  \n            else:\n                model.eval()   \n                \n            running_loss = 0.0\n            running_corrects = 0\n            \n            \n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                \n                optimizer.zero_grad()\n                \n                # Forward pass\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    \n                    # Backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                \n                \n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            \n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n            \n            \n            if phase == 'train':\n                train_losses.append(epoch_loss)\n                train_accs.append(epoch_acc.item())\n            else:\n                val_losses.append(epoch_loss)\n                val_accs.append(epoch_acc.item())\n            \n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n            \n            \n            if phase == 'val':\n                \n                scheduler.step(epoch_loss)\n                \n                \n                if epoch_acc > best_acc:\n                    best_acc = epoch_acc\n                    best_model_wts = model.state_dict().copy()\n                \n                \n                if epoch_loss < best_val_loss:\n                    best_val_loss = epoch_loss\n                    early_stopping_counter = 0\n                else:\n                    early_stopping_counter += 1\n                    if early_stopping_counter >= early_stopping_patience:\n                        print(f'Early stopping triggered after {epoch+1} epochs')\n                        \n                        model.load_state_dict(best_model_wts)\n                        \n                        \n                        plot_learning_curves(train_losses, val_losses, train_accs, val_accs)\n                        \n                        \n                        check_fitting(train_losses, val_losses)\n                        \n                        time_elapsed = time.time() - since\n                        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n                        print(f'Best val Acc: {best_acc:4f}')\n                        \n                        return model, train_losses, val_losses, train_accs, val_accs\n        \n        print()\n    \n    \n    model.load_state_dict(best_model_wts)\n    \n    \n    plot_learning_curves(train_losses, val_losses, train_accs, val_accs)\n    \n    \n    check_fitting(train_losses, val_losses)\n    \n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val Acc: {best_acc:4f}')\n    \n    return model, train_losses, val_losses, train_accs, val_accs\n\n\ndef check_fitting(train_losses, val_losses):\n    \n    final_train_loss = train_losses[-1]\n    final_val_loss = val_losses[-1]\n    loss_gap = final_val_loss - final_train_loss\n    \n    print(\"\\n--- Fitting Analysis ---\")\n    \n    \n    if final_train_loss > 0.3:  \n        print(\"Model may be UNDERFITTING:\")\n       \n    \n    \n    elif loss_gap > 0.1:  \n        print(\"Model may be OVERFITTING:\")\n       \n    \n    else:\n        print(\"Model seems to be fitting well!\")\n        \n\n\ndef plot_learning_curves(train_losses, val_losses, train_accs, val_accs):\n    plt.figure(figsize=(12, 5))\n    \n    \n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label='Training Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.title('Loss Curves')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    \n    plt.subplot(1, 2, 2)\n    plt.plot(train_accs, label='Training Accuracy')\n    plt.plot(val_accs, label='Validation Accuracy')\n    plt.title('Accuracy Curves')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.tight_layout()\n    \n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    plt.savefig(f'learning_curves_{timestamp}.png')\n    plt.show()\n\n\ndef evaluate_model(model, test_loader):\n    model.eval()\n    \n    \n    all_preds = []\n    all_labels = []\n    running_corrects = 0\n    \n    \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            \n            \n            running_corrects += torch.sum(preds == labels.data)\n    \n    \n    test_acc = running_corrects.double() / dataset_sizes['test']\n    print(f'Test Accuracy: {test_acc:.4f}')\n    \n    return all_preds, all_labels, test_acc.item()\n\n\ndef plot_confusion_matrix(y_true, y_pred, classes):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    \n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    plt.savefig(f'confusion_matrix_{timestamp}.png')\n    plt.show()\n\n\ndef plot_roc_curve(y_test, y_score, classes):\n    # Binarize the labels for multi-class ROC\n    y_test_bin = label_binarize(y_test, classes=range(len(classes)))\n    n_classes = y_test_bin.shape[1]\n    \n    \n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    \n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n    \n    \n    plt.figure(figsize=(10, 8))\n    colors = cycle(['blue', 'red', 'green', 'yellow', 'purple', 'orange', 'brown'])\n    \n    for i, color in zip(range(n_classes), colors):\n        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n                 label=f'ROC curve of class {classes[i]} (area = {roc_auc[i]:.2f})')\n    \n    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Multi-class ROC Curve')\n    plt.legend(loc=\"lower right\")\n    \n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    plt.savefig(f'roc_curve_{timestamp}.png')\n    plt.show()\n\n\ndef generate_classification_report(y_true, y_pred, classes):\n    report = classification_report(y_true, y_pred, target_names=classes, output_dict=True)\n    \n    \n    import pandas as pd\n    df_report = pd.DataFrame(report).transpose()\n    \n    \n    plt.figure(figsize=(12, 8))\n    sns.heatmap(df_report.iloc[:-3, :3], annot=True, cmap='Blues', fmt='.2f')\n    plt.title('Classification Report')\n    \n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    plt.savefig(f'classification_report_{timestamp}.png')\n    plt.show()\n    \n    \n    print(\"\\nClassification Report:\")\n    print(classification_report(y_true, y_pred, target_names=classes))\n\n\nif __name__ == \"__main__\":\n    \n    print(\"Starting model training...\")\n    model, train_losses, val_losses, train_accs, val_accs = train_model(\n        model, criterion, optimizer, scheduler, num_epochs=50, early_stopping_patience=7\n    )\n    \n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    torch.save(model.state_dict(), f'aquascan_model_{timestamp}.pth')\n    print(f\"Model saved as aquascan_model_{timestamp}.pth\")\n    \n    \n    print(\"\\nEvaluating model on test set...\")\n    all_preds, all_labels, test_acc = evaluate_model(model, dataloaders['test'])\n    \n    \n    print(\"\\nGenerating confusion matrix...\")\n    plot_confusion_matrix(all_labels, all_preds, class_names)\n    \n    \n    print(\"\\nGenerating ROC curve...\")\n    \n    model.eval()\n    y_score = []\n    with torch.no_grad():\n        for inputs, _ in dataloaders['test']:\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            probs = torch.nn.functional.softmax(outputs, dim=1)\n            y_score.extend(probs.cpu().numpy())\n    \n    plot_roc_curve(all_labels, np.array(y_score), class_names)\n    \n    \n    print(\"\\nGenerating classification report...\")\n    generate_classification_report(all_labels, all_preds, class_names)\n    \n    print(\"\\nAll analysis complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T07:43:36.580827Z","iopub.execute_input":"2025-04-12T07:43:36.581114Z","iopub.status.idle":"2025-04-12T07:49:49.460574Z","shell.execute_reply.started":"2025-04-12T07:43:36.581096Z","shell.execute_reply":"2025-04-12T07:49:49.459429Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****Hybrid","metadata":{}},{"cell_type":"code","source":"\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndata_dir = '/kaggle/working/'  \n\n\ndef analyze_dataset(data_dir):\n    print(\"\\n=== DATASET ANALYSIS ===\")\n    \n    \n    class_counts = {}\n    for split in ['train', 'val', 'test']:\n        split_dir = os.path.join(data_dir, split)\n        if not os.path.exists(split_dir):\n            print(f\"WARNING: {split_dir} directory does not exist!\")\n            continue\n            \n        \n        split_class_counts = {}\n        for class_name in os.listdir(split_dir):\n            class_dir = os.path.join(split_dir, class_name)\n            if os.path.isdir(class_dir):\n                num_images = len([f for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))])\n                split_class_counts[class_name] = num_images\n        \n        print(f\"\\n{split.upper()} set:\")\n        for class_name, count in split_class_counts.items():\n            print(f\"  - {class_name}: {count} images\")\n        \n        total = sum(split_class_counts.values())\n        print(f\"  Total: {total} images\")\n        \n        \n        if split_class_counts:\n            min_count = min(split_class_counts.values())\n            max_count = max(split_class_counts.values())\n            imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n            print(f\"  Class imbalance ratio (max/min): {imbalance_ratio:.2f}\")\n            if imbalance_ratio > 2:\n                print(\"  WARNING: Significant class imbalance detected!\")\n        \n        if split == 'train':\n            class_counts = split_class_counts\n    \n    print(\"\\n=== END OF DATASET ANALYSIS ===\\n\")\n    return class_counts\n\n\nclass_counts = analyze_dataset(data_dir)\n\n\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomVerticalFlip(),\n        transforms.RandomRotation(15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.1),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n}\n\n\ntry:\n    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) \n                    for x in ['train', 'val', 'test']}\n    \n    \n    if class_counts:\n        class_weights = [1.0 / class_counts[image_datasets['train'].classes[i]] \n                        for i in range(len(image_datasets['train'].classes))]\n        sample_weights = [class_weights[label] for _, label in image_datasets['train'].samples]\n        sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n        \n        \n        dataloaders = {\n            'train': DataLoader(image_datasets['train'], batch_size=16, sampler=sampler, num_workers=4),\n            'val': DataLoader(image_datasets['val'], batch_size=16, shuffle=False, num_workers=4),\n            'test': DataLoader(image_datasets['test'], batch_size=16, shuffle=False, num_workers=4)\n        }\n    else:\n        dataloaders = {x: DataLoader(image_datasets[x], batch_size=16, shuffle=(x == 'train'), num_workers=4)\n                    for x in ['train', 'val', 'test']}\n    \n    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n    class_names = image_datasets['train'].classes\n    num_classes = len(class_names)\n    \n    print(f\"Classes: {class_names}\")\n    print(f\"Number of training images: {dataset_sizes['train']}\")\n    print(f\"Number of validation images: {dataset_sizes['val']}\")\n    print(f\"Number of test images: {dataset_sizes['test']}\")\n    \n    \n    def visualize_batch(dataloader, title):\n        batch = next(iter(dataloader))\n        images, labels = batch\n        \n        \n        grid = vutils.make_grid(images[:16], nrow=4, normalize=True)\n        plt.figure(figsize=(10, 10))\n        plt.title(title)\n        plt.imshow(grid.permute(1, 2, 0).cpu())\n        plt.axis('off')\n        plt.show()\n        \n        \n        print(\"Labels for visualized images:\")\n        for i, label in enumerate(labels[:16]):\n            print(f\"Image {i+1}: {class_names[label]}\")\n    \n    print(\"\\nVisualizing training images:\")\n    visualize_batch(dataloaders['train'], \"Training Images\")\n    \nexcept Exception as e:\n    print(f\"Error loading datasets: {e}\")\n    raise\n\n# Define the AquaScan-EfficientNet hybrid model\nclass AquaScanEfficientNetHybrid(nn.Module):\n    def __init__(self, num_classes=7, model_name='efficientnet_b0', use_pretrained=True):\n        super(AquaScanEfficientNetHybrid, self).__init__()\n        \n        # Load pre-trained EfficientNet\n        if model_name == 'efficientnet_b0':\n            self.backbone = models.efficientnet_b0(weights='IMAGENET1K_V1' if use_pretrained else None)\n        elif model_name == 'efficientnet_b1':\n            self.backbone = models.efficientnet_b1(weights='IMAGENET1K_V1' if use_pretrained else None)\n        elif model_name == 'efficientnet_b2':\n            self.backbone = models.efficientnet_b2(weights='IMAGENET1K_V1' if use_pretrained else None)\n        else:\n            raise ValueError(f\"Unsupported model name: {model_name}\")\n        \n        \n        self.features = nn.Sequential(*list(self.backbone.children())[:-1])\n        \n        \n        if model_name == 'efficientnet_b0':\n            backbone_features = 1280\n        elif model_name == 'efficientnet_b1':\n            backbone_features = 1280\n        elif model_name == 'efficientnet_b2':\n            backbone_features = 1408\n        \n        \n        self.activation = nn.LeakyReLU(0.1, inplace=True)\n        \n        # Original AquaScan convolutional blocks with residual connections\n        # Block 1 with residual connection\n        self.conv_block1_main = nn.Sequential(\n            nn.Conv2d(backbone_features, backbone_features, kernel_size=3, padding=1),\n            nn.BatchNorm2d(backbone_features),\n            self.activation,\n            nn.Conv2d(backbone_features, backbone_features, kernel_size=3, padding=1),\n            nn.BatchNorm2d(backbone_features),\n            nn.Dropout(0.1)\n        )\n        \n        # Block 2 with residual connection\n        self.conv_block2_main = nn.Sequential(\n            nn.Conv2d(backbone_features, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            self.activation,\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.Dropout(0.1)\n        )\n        self.conv_block2_shortcut = nn.Sequential(\n            nn.Conv2d(backbone_features, 512, kernel_size=1, stride=1),\n            nn.BatchNorm2d(512)\n        )\n        \n        # Global Average Pooling\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        \n        # Fully Connected Blocks with more capacity\n        self.fc_block = nn.Sequential(\n            nn.Linear(512, 1024),\n            self.activation,\n            nn.Dropout(0.3),\n            nn.Linear(1024, 512),\n            self.activation,\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            self.activation,\n            nn.Dropout(0.3)\n        )\n        \n        # Output Layer\n        self.output_layer = nn.Linear(256, num_classes)\n        \n        # Initially freeze the backbone\n        self.freeze_backbone()\n        \n    def freeze_backbone(self):\n        for param in self.features.parameters():\n            param.requires_grad = False\n            \n    def unfreeze_backbone(self):\n        for param in self.features.parameters():\n            param.requires_grad = True\n    \n    def unfreeze_layers(self, num_layers):\n        # Unfreeze the last num_layers of the backbone\n        children = list(self.features.children())\n        for child in children[-num_layers:]:\n            for param in child.parameters():\n                param.requires_grad = True\n    \n    def forward(self, x):\n        # Get features from EfficientNet backbone\n        x = self.features(x)\n        \n        # Block 1 with residual connection\n        identity = x\n        x = self.conv_block1_main(x)\n        x = x + identity  # Residual connection\n        x = self.activation(x)\n        \n        # Block 2 with residual connection\n        identity = self.conv_block2_shortcut(x)\n        x = self.conv_block2_main(x)\n        x = x + identity  # Residual connection\n        x = self.activation(x)\n        \n        # Global Average Pooling\n        x = self.gap(x)\n        x = x.view(x.size(0), -1)\n        \n        # Fully connected layers\n        x = self.fc_block(x)\n        x = self.output_layer(x)\n        \n        return x\n\n\nmodel = AquaScanEfficientNetHybrid(num_classes=num_classes, model_name='efficientnet_b0').to(device)\nprint(model)\n\n\ndef count_trainable_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ntotal_params = count_trainable_parameters(model)\nprint(f\"Total Trainable Parameters: {total_params}\")\n\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n\noptimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n                       lr=0.001, weight_decay=1e-4)\n\n\nsteps_per_epoch = len(dataloaders['train'])\ntotal_steps = steps_per_epoch * 100  # 100 epochs max\n\n\nscheduler = OneCycleLR(\n    optimizer,\n    max_lr=0.01,\n    total_steps=total_steps,\n    pct_start=0.1,\n    div_factor=25,\n    final_div_factor=1000\n)\n\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=100, early_stopping_patience=10):\n    since = time.time()\n    \n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    \n    train_losses = []\n    val_losses = []\n    train_accs = []\n    val_accs = []\n    \n    \n    best_val_loss = float('inf')\n    early_stopping_counter = 0\n    \n    \n    unfreeze_schedule = {\n        10: 2,   \n        20: 4,   \n        30: 6    \n    }\n    \n    # Training loop\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print('-' * 10)\n        \n        \n        if epoch in unfreeze_schedule:\n            print(f\"Unfreezing the last {unfreeze_schedule[epoch]} layers of the backbone\")\n            model.unfreeze_layers(unfreeze_schedule[epoch])\n            \n            \n            optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n                                  lr=0.0005, weight_decay=1e-4)\n            \n            \n            remaining_epochs = num_epochs - epoch\n            total_steps = steps_per_epoch * remaining_epochs\n            \n            \n            scheduler = OneCycleLR(\n                optimizer,\n                max_lr=0.001,  \n                total_steps=total_steps,\n                pct_start=0.1,\n                div_factor=25,\n                final_div_factor=1000\n            )\n        \n        \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  \n            else:\n                model.eval()   \n                \n            running_loss = 0.0\n            running_corrects = 0\n            all_preds = []\n            all_labels = []\n            \n            \n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                \n                optimizer.zero_grad()\n                \n                # Forward pass\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    \n                    # Backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        # Gradient clipping\n                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                        optimizer.step()\n                        scheduler.step()\n                \n                \n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n                \n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n            \n            \n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n            \n            \n            per_class_acc = {}\n            for i, class_name in enumerate(class_names):\n                class_mask = np.array(all_labels) == i\n                if np.sum(class_mask) > 0:  # Avoid division by zero\n                    class_correct = np.sum((np.array(all_preds) == i) & class_mask)\n                    per_class_acc[class_name] = class_correct / np.sum(class_mask)\n                else:\n                    per_class_acc[class_name] = 0.0\n            \n            \n            if phase == 'train':\n                train_losses.append(epoch_loss)\n                train_accs.append(epoch_acc.item())\n            else:\n                val_losses.append(epoch_loss)\n                val_accs.append(epoch_acc.item())\n            \n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n            \n            \n            print(\"Per-class accuracy:\")\n            for class_name, acc in per_class_acc.items():\n                print(f\"  {class_name}: {acc:.4f}\")\n            \n            \n            if phase == 'val':\n                if epoch_acc > best_acc:\n                    best_acc = epoch_acc\n                    best_model_wts = copy.deepcopy(model.state_dict())\n                    \n                    early_stopping_counter = 0\n                \n                \n                if epoch_loss < best_val_loss:\n                    best_val_loss = epoch_loss\n                else:\n                    early_stopping_counter += 1\n                    if early_stopping_counter >= early_stopping_patience:\n                        print(f'Early stopping triggered after {epoch+1} epochs')\n                        \n                        model.load_state_dict(best_model_wts)\n                        \n                        \n                        plot_learning_curves(train_losses, val_losses, train_accs, val_accs)\n                        \n                        time_elapsed = time.time() - since\n                        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n                        print(f'Best val Acc: {best_acc:4f}')\n                        \n                        return model, train_losses, val_losses, train_accs, val_accs\n        \n        \n        for param_group in optimizer.param_groups:\n            print(f'Current learning rate: {param_group[\"lr\"]:.6f}')\n        \n        print()\n    \n    \n    model.load_state_dict(best_model_wts)\n    \n    \n    plot_learning_curves(train_losses, val_losses, train_accs, val_accs)\n    \n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val Acc: {best_acc:4f}')\n    \n    return model, train_losses, val_losses, train_accs, val_accs\n\n\ndef plot_learning_curves(train_losses, val_losses, train_accs, val_accs):\n    plt.figure(figsize=(12, 5))\n    \n    \n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label='Training Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.title('Loss Curves')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    \n    plt.subplot(1, 2, 2)\n    plt.plot(train_accs, label='Training Accuracy')\n    plt.plot(val_accs, label='Validation Accuracy')\n    plt.title('Accuracy Curves')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.tight_layout()\n    \n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    plt.savefig(f'learning_curves_{timestamp}.png')\n    plt.show()\n\n\ndef evaluate_model(model, test_loader):\n    model.eval()\n    \n    \n    all_preds = []\n    all_labels = []\n    all_probs = []\n    running_corrects = 0\n    \n    \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            outputs = model(inputs)\n            probs = F.softmax(outputs, dim=1)\n            _, preds = torch.max(outputs, 1)\n            \n            \n            all_preds.extend(preds.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            \n            \n            running_corrects += torch.sum(preds == labels.data)\n    \n    \n    test_acc = running_corrects.double() / dataset_sizes['test']\n    print(f'Test Accuracy: {test_acc:.4f}')\n    \n    \n    per_class_acc = {}\n    for i, class_name in enumerate(class_names):\n        class_mask = np.array(all_labels) == i\n        if np.sum(class_mask) > 0:  # Avoid division by zero\n            class_correct = np.sum((np.array(all_preds) == i) & class_mask)\n            per_class_acc[class_name] = class_correct / np.sum(class_mask)\n        else:\n            per_class_acc[class_name] = 0.0\n    \n    print(\"Per-class test accuracy:\")\n    for class_name, acc in per_class_acc.items():\n        print(f\"  {class_name}: {acc:.4f}\")\n    \n    return all_preds, all_probs, all_labels, test_acc.item()\n\n\ndef plot_confusion_matrix(y_true, y_pred, classes):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    \n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    plt.savefig(f'confusion_matrix_{timestamp}.png')\n    plt.show()\n\n\ndef plot_roc_curve(y_test, y_score, classes):\n    \n    y_test_bin = label_binarize(y_test, classes=range(len(classes)))\n    n_classes = y_test_bin.shape[1]\n    \n    \n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    \n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n    \n    \n    plt.figure(figsize=(10, 8))\n    colors = cycle(['blue', 'red', 'green', 'yellow', 'purple', 'orange', 'brown'])\n    \n    for i, color in zip(range(n_classes), colors):\n        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n                 label=f'ROC curve of class {classes[i]} (area = {roc_auc[i]:.2f})')\n    \n    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Multi-class ROC Curve')\n    plt.legend(loc=\"lower right\")\n    \n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    plt.savefig(f'roc_curve_{timestamp}.png')\n    plt.show()\n\n\ndef generate_classification_report(y_true, y_pred, classes):\n    report = classification_report(y_true, y_pred, target_names=classes, output_dict=True)\n    \n    \n    import pandas as pd\n    df_report = pd.DataFrame(report).transpose()\n    \n    \n    plt.figure(figsize=(12, 8))\n    sns.heatmap(df_report.iloc[:-3, :3], annot=True, cmap='Blues', fmt='.2f')\n    plt.title('Classification Report')\n    \n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    plt.savefig(f'classification_report_{timestamp}.png')\n    plt.show()\n    \n    \n    print(\"\\nClassification Report:\")\n    print(classification_report(y_true, y_pred, target_names=classes))\n\n\ndef test_time_augmentation(model, inputs, num_augmentations=5):\n    model.eval()\n    batch_size = inputs.size(0)\n    \n    \n    with torch.no_grad():\n        outputs = model(inputs)\n        probs = F.softmax(outputs, dim=1)\n    \n    \n    all_probs = probs.clone()\n    \n    \n    tta_transforms = [\n        transforms.RandomHorizontalFlip(p=1.0),\n        transforms.RandomVerticalFlip(p=1.0),\n        transforms.RandomRotation(degrees=(90, 90)),\n        transforms.RandomRotation(degrees=(180, 180)),\n        transforms.RandomRotation(degrees=(270, 270)),\n    ]\n    \n    \n    for transform in tta_transforms[:num_augmentations]:\n        \n        aug_inputs = torch.stack([transform(img) for img in inputs])\n        \n        \n        with torch.no_grad():\n            aug_outputs = model(aug_inputs)\n            aug_probs = F.softmax(aug_outputs, dim=1)\n        \n        \n        all_probs += aug_probs\n    \n    \n    avg_probs = all_probs / (num_augmentations + 1)\n    _, preds = torch.max(avg_probs, 1)\n    \n    return preds, avg_probs\n\n\nif __name__ == \"__main__\":\n    \n    print(\"Starting model training...\")\n    model, train_losses, val_losses, train_accs, val_accs = train_model(\n        model, criterion, optimizer, scheduler, num_epochs=100, early_stopping_patience=10\n    )\n    \n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    torch.save(model.state_dict(), f'aquascan_model_{timestamp}.pth')\n    print(f\"Model saved as aquascan_model_{timestamp}.pth\")\n    \n    \n    print(\"\\nEvaluating model on test set \")\n    \n    \n    all_preds = []\n    all_probs = []\n    all_labels = []\n    running_corrects = 0\n    \n    \n    model.eval()\n    for inputs, labels in dataloaders['test']:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        \n        preds, probs = test_time_augmentation(model, inputs)\n        \n        \n        all_preds.extend(preds.cpu().numpy())\n        all_probs.extend(probs.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n        \n        \n        running_corrects += torch.sum(preds == labels.data)\n    \n    \n    test_acc = running_corrects.double() / dataset_sizes['test']\n    print(f'Test Accuracy with TTA: {test_acc:.4f}')\n    \n    \n    print(\"\\nGenerating confusion matrix...\")\n    plot_confusion_matrix(all_labels, all_preds, class_names)\n    \n    \n    print(\"\\nGenerating ROC curve...\")\n    plot_roc_curve(all_labels, np.array(all_probs), class_names)\n    \n    \n    print(\"\\nGenerating classification report...\")\n    generate_classification_report(all_labels, all_preds, class_names)\n    \n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T15:12:13.116608Z","iopub.execute_input":"2025-04-14T15:12:13.116947Z","iopub.status.idle":"2025-04-14T15:13:56.980538Z","shell.execute_reply.started":"2025-04-14T15:12:13.116914Z","shell.execute_reply":"2025-04-14T15:13:56.979483Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****Vision Transformer","metadata":{}},{"cell_type":"code","source":"\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n# Check if CUDA is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n#Checking underfit overfit\ndef diagnose_model_fit(history):\n    \n    # Get final metrics\n    final_train_acc = history['train_acc'][-1]\n    final_val_acc = history['val_acc'][-1]\n    final_train_loss = history['train_loss'][-1]\n    final_val_loss = history['val_loss'][-1]\n    \n    # Calculate gaps\n    acc_gap = final_train_acc - final_val_acc\n    loss_gap = final_val_loss - final_train_loss\n    \n    # Analyze training curves\n    train_acc_trend = np.array(history['train_acc'])\n    val_acc_trend = np.array(history['val_acc'])\n    \n    # Check if training accuracy is still improving in the last few epochs\n    train_still_improving = train_acc_trend[-1] > np.mean(train_acc_trend[-4:-1]) if len(train_acc_trend) >= 4 else True\n    \n    # Print diagnostics\n    print(\"\\\\n===== MODEL FIT DIAGNOSIS =====\")\n    print(f\"Final Training Accuracy: {final_train_acc:.4f}\")\n    print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n    print(f\"Accuracy Gap (Train-Val): {acc_gap:.4f}\")\n    print(f\"Loss Gap (Val-Train): {loss_gap:.4f}\")\n    \n    # Determine fit status\n    if final_train_acc < 0.9 and train_still_improving:\n        print(\"\\\\nDIAGNOSIS: UNDERFITTING\")\n    \n    elif acc_gap > 0.1 or loss_gap > 0.1:\n        print(\"\\\\nDIAGNOSIS: OVERFITTING\")\n       \n    else:\n        print(\"\\\\nDIAGNOSIS: WELL-FITTED\")\n        \n    \n    # Plot learning curves with fit regions\n    plt.figure(figsize=(12, 5))\n    \n    # Plot accuracy with regions\n    plt.subplot(1, 2, 1)\n    epochs = range(1, len(history['train_acc']) + 1)\n    plt.plot(epochs, history['train_acc'], 'b-', label='Training Acc')\n    plt.plot(epochs, history['val_acc'], 'r-', label='Validation Acc')\n    \n    # Add colored regions to indicate fitting status\n    plt.fill_between(epochs, history['train_acc'], history['val_acc'], \n                    where=(np.array(history['train_acc']) > np.array(history['val_acc']) + 0.1),\n                    color='red', alpha=0.3, label='Overfitting Region')\n    plt.fill_between(epochs, [0.9] * len(epochs), [1.0] * len(epochs),\n                    where=(np.array(history['train_acc']) < 0.9),\n                    color='blue', alpha=0.3, label='Underfitting Region')\n    \n    plt.title('Model Accuracy with Fit Diagnosis')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True)\n    \n    # Plot loss\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n    plt.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n    \n    # Add colored regions to indicate fitting status\n    plt.fill_between(epochs, history['train_loss'], history['val_loss'],\n                    where=(np.array(history['val_loss']) > np.array(history['train_loss']) + 0.1),\n                    color='red', alpha=0.3, label='Overfitting Region')\n    \n    plt.title('Model Loss with Fit Diagnosis')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return {\n        'diagnosis': 'underfitting' if final_train_acc < 0.9 and train_still_improving else \n                    'overfitting' if acc_gap > 0.1 or loss_gap > 0.1 else 'well-fitted',\n        'metrics': {\n            'final_train_acc': final_train_acc,\n            'final_val_acc': final_val_acc,\n            'acc_gap': acc_gap,\n            'loss_gap': loss_gap\n        }\n    }\n\n\n\n# Custom Dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        \n        self.root_dir = root_dir\n        self.transform = transform\n\n        # Check if directory exists\n        if not os.path.exists(root_dir):\n            raise FileNotFoundError(f\"Directory not found: {root_dir}\")\n            \n        self.classes = sorted(os.listdir(root_dir))\n        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n        \n        self.samples = []\n        for class_name in self.classes:\n            class_dir = os.path.join(root_dir, class_name)\n            if not os.path.isdir(class_dir):\n                continue\n                \n            for img_name in os.listdir(class_dir):\n                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n                    self.samples.append((\n                        os.path.join(class_dir, img_name),\n                        self.class_to_idx[class_name]\n                    ))\n        \n        print(f\"Loaded {len(self.samples)} images from {len(self.classes)} classes\")\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        \n        # Load image\n        img = Image.open(img_path).convert('RGB')\n        \n        # Apply transformations\n        if self.transform:\n            img = self.transform(img)\n            \n        return img, label\n\n# Create data transformations with light augmentation\ndef get_transforms():\n   \n    train_transform = transforms.Compose([\n        transforms.Resize((256, 256)),  \n        transforms.RandomCrop(224),     \n        transforms.RandomHorizontalFlip(p=0.3),  \n        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),  \n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        transforms.RandomErasing(p=0.1, scale=(0.02, 0.1))  \n    ])\n    \n    # Validation transform (no augmentation)\n    val_transform = transforms.Compose([\n        transforms.Resize((224, 224)),  \n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    return train_transform, val_transform\n\n\ndef create_vit_model(num_classes=7, fine_tune=True, dropout_rate=0.2):\n    # Load pre-trained ViT model with ImageNet weights\n    weights = ViT_B_16_Weights.DEFAULT\n    model = vit_b_16(weights=weights)\n    \n    print(\"Vision Transformer architecture:\")\n    print(model)\n    print(\"\\\\nUsing transfer learning with ImageNet pre-trained weights\")\n    \n   \n    num_features = model.heads.head.in_features\n    \n   \n    model.heads = nn.Sequential(\n        nn.LayerNorm(num_features),\n        nn.Dropout(dropout_rate), \n        nn.Linear(num_features, 512),  \n        nn.GELU(),  \n        nn.LayerNorm(512),  \n        nn.Dropout(dropout_rate),  \n        nn.Linear(512, num_classes)  \n    )\n    \n    # Freeze early layers if fine_tune is True\n    if fine_tune:\n        # Freeze all parameters\n        for param in model.parameters():\n            param.requires_grad = False\n        \n        # Unfreeze the classifier head\n        for param in model.heads.parameters():\n            param.requires_grad = True\n        \n        # Unfreeze the last transformer block\n        for param in model.encoder.layers[-1].parameters():\n            param.requires_grad = True\n        \n        # Unfreeze layer norm parameters throughout the model\n        \n        for name, param in model.named_parameters():\n            if 'norm' in name:\n                param.requires_grad = True\n    \n    return model\n\n# Training function with mixup data augmentation and gradient clipping\ndef train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25, patience=7, \n                mixup_alpha=0.2, clip_grad_norm=1.0):\n    since = time.time()\n    \n    # Initialize variables\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    best_epoch = 0\n    counter = 0  # For early stopping\n    \n    # For tracking metrics\n    history = {\n        'train_loss': [], 'train_acc': [],\n        'val_loss': [], 'val_acc': []\n    }\n    \n    # Mixup function for data augmentation\n    def mixup_data(x, y, alpha=0.2):\n        '''Returns mixed inputs, pairs of targets, and lambda'''\n        if alpha > 0:\n            lam = np.random.beta(alpha, alpha)\n        else:\n            lam = 1\n\n        batch_size = x.size()[0]\n        index = torch.randperm(batch_size).to(device)\n\n        mixed_x = lam * x + (1 - lam) * x[index, :]\n        y_a, y_b = y, y[index]\n        return mixed_x, y_a, y_b, lam\n\n    def mixup_criterion(criterion, pred, y_a, y_b, lam):\n        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print('-' * 10)\n        \n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  \n            else:\n                model.eval()   \n                \n            running_loss = 0.0\n            running_corrects = 0\n            \n            # Iterate over data\n            for inputs, labels in tqdm(dataloaders[phase], desc=f\"{phase}\"):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                # Zero the parameter gradients\n                optimizer.zero_grad()\n                \n                # Forward pass\n                with torch.set_grad_enabled(phase == 'train'):\n                    # Apply mixup in training phase\n                    if phase == 'train' and mixup_alpha > 0:\n                        inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, mixup_alpha)\n                        outputs = model(inputs)\n                        loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n                        _, preds = torch.max(outputs, 1)\n                    else:\n                        outputs = model(inputs)\n                        loss = criterion(outputs, labels)\n                        _, preds = torch.max(outputs, 1)\n                    \n                    # Backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        \n                        # Gradient clipping to prevent exploding gradients\n                        if clip_grad_norm > 0:\n                            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n                            \n                        optimizer.step()\n                \n                # Statistics\n                running_loss += loss.item() * inputs.size(0)\n                if phase == 'train' and mixup_alpha > 0:\n                    # For mixup, we approximate corrects\n                    running_corrects += (lam * torch.sum(preds == labels_a.data) + \n                                        (1 - lam) * torch.sum(preds == labels_b.data))\n                else:\n                    running_corrects += torch.sum(preds == labels.data)\n            \n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n            \n            # Store history\n            history[f'{phase}_loss'].append(epoch_loss)\n            history[f'{phase}_acc'].append(epoch_acc.item())\n            \n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n            \n            \n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n                best_epoch = epoch\n                counter = 0  \n            elif phase == 'val':\n                counter += 1\n                if counter >= patience:\n                    print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n                    model.load_state_dict(best_model_wts)\n                    time_elapsed = time.time() - since\n                    print(f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n                    print(f\"Best val Acc: {best_acc:.4f} at epoch {best_epoch+1}\")\n                    return model, history\n        \n        # Step the scheduler if in train phase\n        if scheduler is not None:\n            scheduler.step()\n\n    # Load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    time_elapsed = time.time() - since\n    print(f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n    print(f\"Best val Acc: {best_acc:.4f} at epoch {best_epoch+1}\")\n    \n    return model, history\n\n# Evaluate the model\ndef evaluate_model(model, test_loader, class_names):\n    model.eval()\n    \n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Calculate accuracy\n    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n    print(f\"Test Accuracy: {accuracy:.4f}\")\n    \n    # Create confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    \n    # Plot confusion matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.show()\n    \n    # Print classification report\n    print(\"\\\\nClassification Report:\")\n    print(classification_report(all_labels, all_preds, target_names=class_names))\n    \n    return accuracy, cm\n\n# Plot training history\ndef plot_training_history(history):\n    plt.figure(figsize=(12, 5))\n    \n    # Plot accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history['train_acc'], label='Train')\n    plt.plot(history['val_acc'], label='Validation')\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True)\n    \n    # Plot loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history['train_loss'], label='Train')\n    plt.plot(history['val_loss'], label='Validation')\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Main execution\ndef main():\n    # Kaggle dataset path \n    data_dir = '/kaggle/input/fish-split/Split'  \n    \n    train_dir = os.path.join(data_dir, 'train')\n    val_dir = os.path.join(data_dir, 'val')\n    test_dir = os.path.join(data_dir, 'test')\n    \n    print(f\"Data directory: {data_dir}\")\n    print(f\"Data directory exists: {os.path.exists(data_dir)}\")\n    print(f\"Train directory: {train_dir}\")\n    print(f\"Train directory exists: {os.path.exists(train_dir)}\")\n    \n    # Check if directories exist\n    for dir_path in [train_dir, val_dir, test_dir]:\n        if not os.path.exists(dir_path):\n            print(f\"Warning: Directory not found: {dir_path}\")\n    \n    # Get transforms with light augmentation to help prevent overfitting\n    train_transform, val_transform = get_transforms()\n    \n    # Create datasets\n    train_dataset = CustomDataset(train_dir, transform=train_transform)\n    val_dataset = CustomDataset(val_dir, transform=val_transform)\n    test_dataset = CustomDataset(test_dir, transform=val_transform)\n    \n    # Get class names\n    class_names = train_dataset.classes\n    print(f\"Classes: {class_names}\")\n    \n    # Create dataloaders with a smaller batch size to increase variability\n    batch_size = 16  \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n    \n    # Combine into a dictionary for the training function\n    dataloaders = {\n        'train': train_loader,\n        'val': val_loader\n    }\n    \n    # Create model with dropout for regularization\n    model = create_vit_model(num_classes=len(class_names), fine_tune=True, dropout_rate=0.2)\n    model = model.to(device)\n    \n    # Set up loss function\n    criterion = nn.CrossEntropyLoss()\n    \n    # Set up optimizer with increased weight decay for regularization\n    optimizer = optim.AdamW(\n        filter(lambda p: p.requires_grad, model.parameters()), \n        lr=0.0001, \n        weight_decay=0.05  \n    )\n    \n    # Set up learning rate scheduler \n    from torch.optim.lr_scheduler import OneCycleLR\n    \n    # Calculate total steps for the scheduler\n    total_steps = len(train_loader) * 30  \n    \n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=0.001,\n        total_steps=total_steps,\n        pct_start=0.1,  \n        div_factor=25,  \n        final_div_factor=1000,  \n        anneal_strategy='cos'  \n    )\n    \n    # Train the model with mixup and gradient clipping\n    model, history = train_model(\n        model=model,\n        dataloaders=dataloaders,\n        criterion=criterion,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        num_epochs=30,\n        patience=10,  \n        mixup_alpha=0.2,  \n        clip_grad_norm=1.0  \n    )\n    \n    # Plot training history\n    plot_training_history(history)\n    \n    # Check for overfitting/underfitting\n    fit_diagnosis = diagnose_model_fit(history)\n    print(f\"\\\\nModel fit diagnosis: {fit_diagnosis['diagnosis']}\")\n    \n    # Evaluate the model\n    accuracy, cm = evaluate_model(model, test_loader, class_names)\n    \n    # Save the model\n    torch.save(model.state_dict(), 'vit_fish_disease_model_improved.pth')\n    print(\"Model saved\")\n\n# Call the main function\nif __name__ == \"__main__\":\n    main()\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****VIT Parameter Checking","metadata":{}},{"cell_type":"code","source":"\n# Function to create the Vision Transformer model \ndef create_vit_model(num_classes=7, fine_tune=True, dropout_rate=0.2):\n    # Load pre-trained ViT model with ImageNet weights\n    weights = ViT_B_16_Weights.DEFAULT\n    model = vit_b_16(weights=weights)\n    \n   \n    num_features = model.heads.head.in_features\n    \n    \n    model.heads = nn.Sequential(\n        nn.LayerNorm(num_features),\n        nn.Dropout(dropout_rate),  \n        nn.Linear(num_features, 512),  \n        nn.GELU(),  \n        nn.LayerNorm(512),  \n        nn.Dropout(dropout_rate),  \n        nn.Linear(512, num_classes)  \n    )\n    \n    # Freeze early layers if fine_tune is True\n    if fine_tune:\n        # Freeze all parameters\n        for param in model.parameters():\n            param.requires_grad = False\n        \n        # Unfreeze the classifier head\n        for param in model.heads.parameters():\n            param.requires_grad = True\n        \n        # Unfreeze the last transformer block\n        for param in model.encoder.layers[-1].parameters():\n            param.requires_grad = True\n        \n        # Unfreeze layer norm parameters throughout the model\n        \n        for name, param in model.named_parameters():\n            if 'norm' in name:\n                param.requires_grad = True\n    \n    return model\n\n# Function to analyze model parameters\ndef analyze_model_parameters(model, save_dir=None):\n    \n    print(\"===== MODEL PARAMETER ANALYSIS =====\")\n    \n    \n    if save_dir and not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    \n    # 1. Basic parameter statistics\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    frozen_params = total_params - trainable_params\n    \n    print(f\"Total parameters: {total_params:,}\")\n    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%})\")\n    print(f\"Frozen parameters: {frozen_params:,} ({frozen_params/total_params:.2%})\")\n    \n    # 2. Parameter distribution by layer\n    param_data = []\n    for name, param in model.named_parameters():\n        # Get module name \n        module_name = name.split('.')[0]\n        if len(name.split('.')) > 1:\n            submodule = name.split('.')[1]\n            if submodule.isdigit():  # For indexed layers like encoder.layers.0\n                module_name = f\"{module_name}.{submodule}\"\n        \n        param_data.append({\n            'name': name,\n            'module': module_name,\n            'shape': tuple(param.shape),\n            'size': param.numel(),\n            'trainable': param.requires_grad,\n            'min': float(param.min()),\n            'max': float(param.max()),\n            'mean': float(param.mean()),\n            'std': float(param.std()),\n            'zeros': float((param == 0).sum() / param.numel()),\n            'type': param.dtype\n        })\n    \n    \n    param_df = pd.DataFrame(param_data)\n    \n    # 3. Parameters by module\n    module_stats = param_df.groupby('module').agg({\n        'size': 'sum',\n        'trainable': lambda x: sum(x) / len(x) * 100  \n    }).sort_values('size', ascending=False)\n    \n    module_stats['size_percent'] = module_stats['size'] / total_params * 100\n    \n    print(\"\\nParameters by module:\")\n    print(module_stats)\n    \n    # 4. Visualize parameter distribution by module\n    plt.figure(figsize=(12, 6))\n    \n    # Plot parameter count by module\n    ax1 = plt.subplot(1, 2, 1)\n    bars = ax1.bar(module_stats.index, module_stats['size'], color='skyblue')\n    ax1.set_title('Parameter Count by Module')\n    ax1.set_ylabel('Number of Parameters')\n    ax1.set_xticklabels(module_stats.index, rotation=45, ha='right')\n    \n    \n    for bar in bars:\n        height = bar.get_height()\n        ax1.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height:,.0f}',\n                ha='center', va='bottom', rotation=0)\n    \n    \n    ax2 = plt.subplot(1, 2, 2)\n    bars = ax2.bar(module_stats.index, module_stats['trainable'], color='lightgreen')\n    ax2.set_title('Trainable Parameters by Module (%)')\n    ax2.set_ylabel('Trainable Parameters (%)')\n    ax2.set_xticklabels(module_stats.index, rotation=45, ha='right')\n    ax2.set_ylim(0, 100)\n    \n   \n    for bar in bars:\n        height = bar.get_height()\n        ax2.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height:.1f}%',\n                ha='center', va='bottom', rotation=0)\n    \n    plt.tight_layout()\n    if save_dir:\n        plt.savefig(os.path.join(save_dir, 'parameter_distribution.png'), dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    # 5. Analyze parameter statistics\n    \n    numeric_cols = ['size', 'min', 'max', 'mean', 'std', 'zeros']\n    numeric_df = param_df[numeric_cols]\n    \n    # Correlation matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n    plt.title('Parameter Statistics Correlation')\n    if save_dir:\n        plt.savefig(os.path.join(save_dir, 'parameter_correlation.png'), dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    # 6. Parameter value distribution\n    plt.figure(figsize=(15, 10))\n    \n    \n    sample_layers = param_df.sample(min(10, len(param_df)))\n    \n    for i, (_, row) in enumerate(sample_layers.iterrows()):\n        param_name = row['name']\n        param_tensor = None\n        \n        \n        for name, param in model.named_parameters():\n            if name == param_name:\n                param_tensor = param.detach().cpu().flatten().numpy()\n                break\n        \n        if param_tensor is not None:\n            plt.subplot(3, 4, i+1)\n            sns.histplot(param_tensor, kde=True, bins=50)\n            plt.title(f\"{param_name[:20]}...\" if len(param_name) > 20 else param_name)\n            plt.xlabel('Value')\n            plt.ylabel('Count')\n    \n    plt.tight_layout()\n    if save_dir:\n        plt.savefig(os.path.join(save_dir, 'parameter_distributions.png'), dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    # 7. Check for potential issues\n    print(\"\\nPotential issues:\")\n    \n    \n    has_nan = any(torch.isnan(p).any() for p in model.parameters())\n    has_inf = any(torch.isinf(p).any() for p in model.parameters())\n    print(f\"- NaN values detected: {has_nan}\")\n    print(f\"- Inf values detected: {has_inf}\")\n    \n    \n    dead_params = param_df[param_df['zeros'] > 0.99]\n    if not dead_params.empty:\n        print(f\"- Potential dead parameters (>99% zeros): {len(dead_params)}\")\n        print(dead_params[['name', 'zeros', 'size']])\n    else:\n        print(\"- No dead parameters detected\")\n    \n    \n    extreme_params = param_df[(param_df['max'] > 100) | (param_df['min'] < -100)]\n    if not extreme_params.empty:\n        print(f\"- Parameters with extreme values (>100 or <-100): {len(extreme_params)}\")\n        print(extreme_params[['name', 'min', 'max', 'mean', 'std']])\n    else:\n        print(\"- No extreme parameter values detected\")\n    \n    return {\n        'total_params': total_params,\n        'trainable_params': trainable_params,\n        'frozen_params': frozen_params,\n        'param_df': param_df,\n        'module_stats': module_stats\n    }\n\n# Function to analyze model architecture\ndef analyze_model_architecture(model):\n    \"\"\"\n    Analyze the architecture of a PyTorch model.\n    \"\"\"\n    print(\"\\n===== MODEL ARCHITECTURE ANALYSIS =====\")\n    \n    \n    print(model)\n    \n    \n    layer_counts = {}\n    for name, module in model.named_modules():\n        layer_type = module.__class__.__name__\n        if layer_type in layer_counts:\n            layer_counts[layer_type] += 1\n        else:\n            layer_counts[layer_type] = 1\n    \n    \n    for container in ['Sequential', 'ModuleList', 'ModuleDict']:\n        if container in layer_counts:\n            del layer_counts[container]\n    \n    \n    layer_counts = OrderedDict(sorted(layer_counts.items(), key=lambda x: x[1], reverse=True))\n    \n    print(\"\\nLayer types:\")\n    for layer_type, count in layer_counts.items():\n        print(f\"- {layer_type}: {count}\")\n    \n    \n    plt.figure(figsize=(12, 6))\n    plt.bar(layer_counts.keys(), layer_counts.values(), color='lightblue')\n    plt.title('Model Layer Composition')\n    plt.xlabel('Layer Type')\n    plt.ylabel('Count')\n    plt.xticks(rotation=45, ha='right')\n    \n    \n    for i, (layer_type, count) in enumerate(layer_counts.items()):\n        plt.text(i, count, str(count), ha='center', va='bottom')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return layer_counts\n\n# Function to analyze model weights from a saved model file\ndef analyze_saved_model(model_path, num_classes=7):\n    \"\"\"\n    Analyze a saved model's parameters.\n    \n    Args:\n        model_path: Path to the saved model file\n        num_classes: Number of classes in the model\n    \"\"\"\n    print(f\"\\n===== ANALYZING SAVED MODEL: {model_path} =====\")\n    \n    \n    model = create_vit_model(num_classes=num_classes)\n    \n    \n    try:\n        model.load_state_dict(torch.load(model_path, map_location='cpu'))\n        print(\"Model loaded successfully!\")\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        return None\n    \n    \n    model.eval()\n    \n    \n    arch_analysis = analyze_model_architecture(model)\n    param_analysis = analyze_model_parameters(model)\n    \n    return model, arch_analysis, param_analysis\n\n# Main execution\nif __name__ == \"__main__\":\n    \n    print(\"Creating a new Vision Transformer model...\")\n    model = create_vit_model(num_classes=7)\n    \n    \n    arch_analysis = analyze_model_architecture(model)\n    \n    \n    param_analysis = analyze_model_parameters(model, save_dir='model_analysis')\n    \n   ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****VGG 16","metadata":{}},{"cell_type":"code","source":"\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndef diagnose_model_fit(history):\n    \n    final_train_acc = history['train_acc'][-1]\n    final_val_acc = history['val_acc'][-1]\n    final_train_loss = history['train_loss'][-1]\n    final_val_loss = history['val_loss'][-1]\n    \n    \n    acc_gap = final_train_acc - final_val_acc\n    loss_gap = final_val_loss - final_train_loss\n    \n    \n    train_acc_trend = np.array(history['train_acc'])\n    val_acc_trend = np.array(history['val_acc'])\n    \n    \n    train_still_improving = train_acc_trend[-1] > np.mean(train_acc_trend[-4:-1]) if len(train_acc_trend) >= 4 else True\n    \n    \n    print(\"\\n===== MODEL FIT DIAGNOSIS =====\")\n    print(f\"Final Training Accuracy: {final_train_acc:.4f}\")\n    print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n    print(f\"Accuracy Gap (Train-Val): {acc_gap:.4f}\")\n    print(f\"Loss Gap (Val-Train): {loss_gap:.4f}\")\n    \n    \n    if final_train_acc < 0.9 and train_still_improving:\n        print(\"\\nDIAGNOSIS: UNDERFITTING\")\n    elif acc_gap > 0.1 or loss_gap > 0.1:\n        print(\"\\nDIAGNOSIS: OVERFITTING\")\n    else:\n        print(\"\\nDIAGNOSIS: WELL-FITTED\")\n    \n    \n    plt.figure(figsize=(12, 5))\n    \n    \n    plt.subplot(1, 2, 1)\n    epochs = range(1, len(history['train_acc']) + 1)\n    plt.plot(epochs, history['train_acc'], 'b-', label='Training Acc')\n    plt.plot(epochs, history['val_acc'], 'r-', label='Validation Acc')\n    \n    \n    plt.fill_between(epochs, history['train_acc'], history['val_acc'], \n                    where=(np.array(history['train_acc']) > np.array(history['val_acc']) + 0.1),\n                    color='red', alpha=0.3, label='Overfitting Region')\n    plt.fill_between(epochs, [0.9] * len(epochs), [1.0] * len(epochs),\n                    where=(np.array(history['train_acc']) < 0.9),\n                    color='blue', alpha=0.3, label='Underfitting Region')\n    \n    plt.title('Model Accuracy with Fit Diagnosis')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True)\n    \n    \n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n    plt.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n    \n    \n    plt.fill_between(epochs, history['train_loss'], history['val_loss'],\n                    where=(np.array(history['val_loss']) > np.array(history['train_loss']) + 0.1),\n                    color='red', alpha=0.3, label='Overfitting Region')\n    \n    plt.title('Model Loss with Fit Diagnosis')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return {\n        'diagnosis': 'underfitting' if final_train_acc < 0.9 and train_still_improving else \n                    'overfitting' if acc_gap > 0.1 or loss_gap > 0.1 else 'well-fitted',\n        'metrics': {\n            'final_train_acc': final_train_acc,\n            'final_val_acc': final_val_acc,\n            'acc_gap': acc_gap,\n            'loss_gap': loss_gap\n        }\n    }\n\n# Custom Dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n\n        \n        if not os.path.exists(root_dir):\n            raise FileNotFoundError(f\"Directory not found: {root_dir}\")\n            \n        self.classes = sorted(os.listdir(root_dir))\n        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n        \n        self.samples = []\n        for class_name in self.classes:\n            class_dir = os.path.join(root_dir, class_name)\n            if not os.path.isdir(class_dir):\n                continue\n                \n            for img_name in os.listdir(class_dir):\n                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n                    self.samples.append((\n                        os.path.join(class_dir, img_name),\n                        self.class_to_idx[class_name]\n                    ))\n        \n        print(f\"Loaded {len(self.samples)} images from {len(self.classes)} classes\")\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        \n        \n        img = Image.open(img_path).convert('RGB')\n        \n        \n        if self.transform:\n            img = self.transform(img)\n            \n        return img, label\n\n\ndef get_transforms():\n   \n    train_transform = transforms.Compose([\n        transforms.Resize((256, 256)),  \n        transforms.RandomCrop(224),    \n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomVerticalFlip(p=0.3),\n        transforms.RandomRotation(20),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        transforms.RandomErasing(p=0.3, scale=(0.02, 0.2))  \n    ])\n    \n    \n    val_transform = transforms.Compose([\n        transforms.Resize((224, 224)), \n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    return train_transform, val_transform\n\n\nclass CustomVGG16(nn.Module):\n    def __init__(self, num_classes=7, dropout_rate=0.3):\n        super(CustomVGG16, self).__init__()\n        \n        \n        vgg16 = models.vgg16(pretrained=True)\n        \n        \n        self.features = vgg16.features\n        \n        \n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        \n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(512 * 7 * 7, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(512, num_classes)\n        )\n        \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = self.classifier(x)\n        return x\n\n\ndef create_vgg16_model(num_classes=7, fine_tune=True, dropout_rate=0.3):\n   \n    model = CustomVGG16(num_classes=num_classes, dropout_rate=dropout_rate)\n    \n    print(\"Custom VGG16 architecture created\")\n    print(\"\\nUsing transfer learning with ImageNet pre-trained weights\")\n    \n    \n    if fine_tune:\n        for param in model.features[:17].parameters():  \n            param.requires_grad = False\n        \n        \n        for param in model.features[17:].parameters():\n            param.requires_grad = True\n        \n        \n        for param in model.classifier.parameters():\n            param.requires_grad = True\n    \n    return model\n\n\ndef train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25, patience=10, \n                mixup_alpha=0.2, clip_grad_norm=1.0):\n    since = time.time()\n    \n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    best_epoch = 0\n    counter = 0  # For early stopping\n    \n   \n    history = {\n        'train_loss': [], 'train_acc': [],\n        'val_loss': [], 'val_acc': []\n    }\n    \n    \n    def mixup_data(x, y, alpha=0.2):\n        \n        if alpha > 0:\n            lam = np.random.beta(alpha, alpha)\n        else:\n            lam = 1\n\n        batch_size = x.size()[0]\n        index = torch.randperm(batch_size).to(device)\n\n        mixed_x = lam * x + (1 - lam) * x[index, :]\n        y_a, y_b = y, y[index]\n        return mixed_x, y_a, y_b, lam\n\n    def mixup_criterion(criterion, pred, y_a, y_b, lam):\n        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print('-' * 10)\n        \n        \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  \n            else:\n                model.eval()   \n                \n            running_loss = 0.0\n            running_corrects = 0\n            \n            \n            for inputs, labels in tqdm(dataloaders[phase], desc=f\"{phase}\"):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                \n                optimizer.zero_grad()\n                \n               \n                with torch.set_grad_enabled(phase == 'train'):\n                    \n                    if phase == 'train' and mixup_alpha > 0:\n                        inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, mixup_alpha)\n                        outputs = model(inputs)\n                        loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n                        _, preds = torch.max(outputs, 1)\n                    else:\n                        outputs = model(inputs)\n                        loss = criterion(outputs, labels)\n                        _, preds = torch.max(outputs, 1)\n                    \n                    \n                    if phase == 'train':\n                        loss.backward()\n                        \n                        \n                        if clip_grad_norm > 0:\n                            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n                            \n                        optimizer.step()\n                \n                \n                running_loss += loss.item() * inputs.size(0)\n                if phase == 'train' and mixup_alpha > 0:\n                    \n                    running_corrects += (lam * torch.sum(preds == labels_a.data) + \n                                        (1 - lam) * torch.sum(preds == labels_b.data))\n                else:\n                    running_corrects += torch.sum(preds == labels.data)\n            \n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n            \n            \n            history[f'{phase}_loss'].append(epoch_loss)\n            history[f'{phase}_acc'].append(epoch_acc.item())\n            \n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n            \n            \n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n                best_epoch = epoch\n                counter = 0  \n            elif phase == 'val':\n                counter += 1\n                if counter >= patience:\n                    print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n                    model.load_state_dict(best_model_wts)\n                    time_elapsed = time.time() - since\n                    print(f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n                    print(f\"Best val Acc: {best_acc:.4f} at epoch {best_epoch+1}\")\n                    return model, history\n        \n        \n        if scheduler is not None:\n            scheduler.step()\n\n    \n    model.load_state_dict(best_model_wts)\n    \n    time_elapsed = time.time() - since\n    print(f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n    print(f\"Best val Acc: {best_acc:.4f} at epoch {best_epoch+1}\")\n    \n    return model, history\n\n\ndef evaluate_model(model, test_loader, class_names):\n    model.eval()\n    \n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    \n    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n    print(f\"Test Accuracy: {accuracy:.4f}\")\n    \n    \n    cm = confusion_matrix(all_labels, all_preds)\n    \n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.show()\n    \n    \n    print(\"\\nClassification Report:\")\n    print(classification_report(all_labels, all_preds, target_names=class_names))\n    \n    return accuracy, cm\n\n\ndef plot_training_history(history):\n    plt.figure(figsize=(12, 5))\n    \n    \n    plt.subplot(1, 2, 1)\n    plt.plot(history['train_acc'], label='Train')\n    plt.plot(history['val_acc'], label='Validation')\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True)\n    \n   \n    plt.subplot(1, 2, 2)\n    plt.plot(history['train_loss'], label='Train')\n    plt.plot(history['val_loss'], label='Validation')\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Main execution\ndef main():\n    \n    data_dir = '/kaggle/input/fish-split/Split'  \n    \n    \n    \n    train_dir = os.path.join(data_dir, 'train')\n    val_dir = os.path.join(data_dir, 'val')\n    test_dir = os.path.join(data_dir, 'test')\n    \n    print(f\"Data directory: {data_dir}\")\n    print(f\"Data directory exists: {os.path.exists(data_dir)}\")\n    print(f\"Train directory: {train_dir}\")\n    print(f\"Train directory exists: {os.path.exists(train_dir)}\")\n    \n    \n    for dir_path in [train_dir, val_dir, test_dir]:\n        if not os.path.exists(dir_path):\n            print(f\"Warning: Directory not found: {dir_path}\")\n    \n    \n    train_transform, val_transform = get_transforms()\n    \n    \n    train_dataset = CustomDataset(train_dir, transform=train_transform)\n    val_dataset = CustomDataset(val_dir, transform=val_transform)\n    test_dataset = CustomDataset(test_dir, transform=val_transform)\n    \n    \n    class_names = train_dataset.classes\n    print(f\"Classes: {class_names}\")\n    \n    \n    batch_size = 16  \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n    \n    \n    dataloaders = {\n        'train': train_loader,\n        'val': val_loader\n    }\n    \n    \n    model = create_vgg16_model(num_classes=len(class_names), fine_tune=True, dropout_rate=0.5)\n    model = model.to(device)\n    \n    \n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  \n    \n    \n    optimizer = optim.AdamW(\n        filter(lambda p: p.requires_grad, model.parameters()), \n        lr=0.0001, \n        weight_decay=0.1  \n    )\n    \n    # Set up learning rate scheduler \n    from torch.optim.lr_scheduler import OneCycleLR\n    \n    \n    total_steps = len(train_loader) * 40  \n    \n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=0.001,  \n        total_steps=total_steps,\n        pct_start=0.1,  \n        div_factor=25, \n        final_div_factor=1000,  \n        anneal_strategy='cos'  \n    )\n    \n    \n    model, history = train_model(\n        model=model,\n        dataloaders=dataloaders,\n        criterion=criterion,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        num_epochs=40,  \n        patience=10,  \n        mixup_alpha=0.3,  \n        \n    )\n    \n    \n    plot_training_history(history)\n    \n    # Check for overfitting/underfitting\n    fit_diagnosis = diagnose_model_fit(history)\n    print(f\"\\nModel fit diagnosis: {fit_diagnosis['diagnosis']}\")\n    \n    \n    accuracy, cm = evaluate_model(model, test_loader, class_names)\n    \n    # Save the model\n    torch.save(model.state_dict(), 'vgg16_fish_disease_model_improved.pth')\n    print(\"Model saved\")\n\n# Call the main function\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****VGG 16 parameter checking","metadata":{}},{"cell_type":"code","source":"\n\nclass CustomVGG16(nn.Module):\n    def __init__(self, num_classes=7, dropout_rate=0.3):\n        super(CustomVGG16, self).__init__()\n        \n        # Load pre-trained VGG16 model\n        vgg16 = models.vgg16(pretrained=True)\n        \n        # Use the convolutional layers from VGG16\n        self.features = vgg16.features\n        \n        # Create a custom classifier with regularization\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        \n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(512 * 7 * 7, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(512, num_classes)\n        )\n        \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = self.classifier(x)\n        return x\n\n# Function to count parameters\ndef count_parameters(model):\n    \"\"\"Count the total and trainable parameters in the model\"\"\"\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return total_params, trainable_params\n\n# Create the model\nmodel = CustomVGG16(num_classes=7, dropout_rate=0.5)\n\n# Count parameters\ntotal_params, trainable_params = count_parameters(model)\n\n# Get original VGG16 parameters for comparison\noriginal_vgg16 = models.vgg16(pretrained=True)\noriginal_total, _ = count_parameters(original_vgg16)\n\n\nprint(f\"Your CustomVGG16 Model:\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")\nprint(\"\\nBreakdown by layer:\")\n\n# Detailed parameter breakdown\nprint(\"\\nFeatures (Convolutional layers):\")\nfeatures_params = sum(p.numel() for p in model.features.parameters())\nprint(f\"  Parameters: {features_params:,}\")\n\nprint(\"\\nClassifier (Fully connected layers):\")\nfor i, layer in enumerate(model.classifier):\n    if hasattr(layer, 'weight'):\n        layer_params = sum(p.numel() for p in layer.parameters())\n        print(f\"  Layer {i} ({layer.__class__.__name__}): {layer_params:,} parameters\")\n\nprint(f\"\\nOriginal VGG16 total parameters: {original_total:,}\")\nprint(f\"Difference: {total_params - original_total:,} parameters\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****Gradcam on VGG 16","metadata":{}},{"cell_type":"code","source":"\n# Define your CustomVGG16 class \nclass CustomVGG16(nn.Module):\n    def __init__(self, num_classes=7, dropout_rate=0.3):\n        super(CustomVGG16, self).__init__()\n        \n        # Load pre-trained VGG16 model\n        vgg16 = models.vgg16(pretrained=True)\n        \n        # Use the convolutional layers from VGG16\n        self.features = vgg16.features\n        \n        # Create a custom classifier with regularization\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        \n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(512 * 7 * 7, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(512, num_classes)\n        )\n        \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = self.classifier(x)\n        return x\n\n# Define the GradCAM class\nclass GradCAM:\n    def __init__(self, model, target_layer):\n        self.model = model\n        self.target_layer = target_layer\n        self.gradients = None\n        self.activations = None\n        \n        # Register hooks\n        def forward_hook(module, input, output):\n            self.activations = output.detach()\n        \n        def backward_hook(module, grad_input, grad_output):\n            self.gradients = grad_output[0].detach()\n        \n        # Register hooks\n        self.target_layer.register_forward_hook(forward_hook)\n        self.target_layer.register_backward_hook(backward_hook)\n        \n        # Set model to evaluation mode\n        self.model.eval()\n    \n    def __call__(self, input_image, target_class=None):\n        # Forward pass\n        output = self.model(input_image)\n        \n        # Get the class index\n        if target_class is None:\n            pred = output.argmax(dim=1)[0]\n            class_idx = pred.item()\n        else:\n            class_idx = target_class\n        \n        # Get the prediction probability\n        pred_prob = F.softmax(output, dim=1)[0, class_idx].item()\n        \n        # Zero gradients\n        self.model.zero_grad()\n        \n        # Backward pass with the target class\n        target = output[0, class_idx]\n        target.backward()\n        \n        # Get the gradients and activations\n        gradients = self.gradients\n        activations = self.activations\n        \n        # Global average pooling of gradients\n        weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n        \n        # Weighted sum of activation maps\n        cam = torch.sum(weights * activations, dim=1, keepdim=True)\n        \n        # ReLU to keep only positive contributions\n        cam = F.relu(cam)\n        \n        # Normalize the CAM\n        cam = F.interpolate(cam, size=input_image.shape[2:], mode='bilinear', align_corners=False)\n        cam = cam - cam.min()\n        cam = cam / (cam.max() + 1e-8)\n        \n        # Convert to numpy\n        heatmap = cam[0, 0].cpu().numpy()\n        \n        return heatmap, class_idx, pred_prob\n\n# Function to preprocess images\ndef preprocess_image(image_path, transform=None):\n    # Load image\n    img = Image.open(image_path).convert('RGB')\n    original_img = np.array(img)\n    \n    # Apply transformations\n    if transform is None:\n        transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    \n    preprocessed_img = transform(img).unsqueeze(0)\n    \n    return preprocessed_img, original_img\n\n# Function to visualize Grad-CAM\ndef visualize_gradcam(original_img, heatmap, class_name, pred_prob, alpha=0.5):\n    # Convert heatmap to RGB\n    heatmap_rgb = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n    heatmap_rgb = cv2.cvtColor(heatmap_rgb, cv2.COLOR_BGR2RGB)\n    \n    # Resize heatmap to match original image\n    heatmap_rgb = cv2.resize(heatmap_rgb, (original_img.shape[1], original_img.shape[0]))\n    \n    # Overlay heatmap on original image\n    superimposed_img = heatmap_rgb * alpha + original_img * (1 - alpha)\n    superimposed_img = np.uint8(superimposed_img)\n    \n    # Create figure with original and overlaid images\n    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Original image\n    ax[0].imshow(original_img)\n    ax[0].set_title('Original Image')\n    ax[0].axis('off')\n    \n    # Overlaid image\n    ax[1].imshow(superimposed_img)\n    ax[1].set_title(f'Grad-CAM: {class_name} ({pred_prob:.2%})')\n    ax[1].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return superimposed_img\n\n# Function to apply Grad-CAM to an image\ndef apply_gradcam(model, image_path, class_names, transform=None, target_class=None):\n    # Preprocess image\n    input_tensor, original_img = preprocess_image(image_path, transform)\n    \n    # Move to device\n    device = next(model.parameters()).device\n    input_tensor = input_tensor.to(device)\n    \n    # Get target layer (last convolutional layer in VGG16)\n    target_layer = model.features[-1]\n    \n    # Initialize Grad-CAM\n    grad_cam = GradCAM(model, target_layer)\n    \n    # Generate heatmap\n    heatmap, class_idx, pred_prob = grad_cam(input_tensor, target_class)\n    \n    # Get class name\n    class_name = class_names[class_idx] if class_idx < len(class_names) else f\"Class {class_idx}\"\n    \n    # Visualize\n    superimposed_img = visualize_gradcam(original_img, heatmap, class_name, pred_prob)\n    \n    print(f\"Predicted class: {class_name} with probability {pred_prob:.2%}\")\n    \n    return superimposed_img, class_idx, pred_prob\n\n# MAIN \n\n# 1. Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# 2. Create model instance\nmodel = CustomVGG16(num_classes=7)  # Adjust number of classes as needed\n\n# 3. Load saved model weights\n\nimport glob\nmodel_paths = glob.glob(\"/kaggle/input/notebooknewfish54601c88a6/vgg16_fish_disease_model_improved.pth\", recursive=True)\n\nif model_paths:\n    model_path = model_paths[0]\n    print(f\"Found model at: {model_path}\")\n    \n   \n    try:\n        model.load_state_dict(torch.load(model_path, map_location=device))\n        print(\"Model loaded successfully!\")\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\nelse:\n    print(\"Model file not found. Please upload it to Kaggle.\")\n# 4. Move model to device and set to evaluation mode\nmodel = model.to(device)\nmodel.eval()\n\n# 5. Define class names \nclass_names = ['Bacterial Red disease',\n            'Bacterial diseases - Aeromoniasis',\n            'Bacterial gill disease', \n            'Fungal diseases Saprolegniasis', \n            'Healthy Fish', \n            'Parasitic diseases', \n            'Viral diseases White tail disease' ]\n\n# 6. Apply Grad-CAM to an image\nimage_path =  '/kaggle/input/fish-split/Split/val/Viral diseases White tail disease/Viral diseases White tail disease (21).jpg'\nsuperimposed_img, class_idx, pred_prob = apply_gradcam(model, image_path, class_names)\n\n# 7. Optional: Apply Grad-CAM to multiple images\ndef process_multiple_images(model, image_paths, class_names):\n    results = []\n    for img_path in image_paths:\n        print(f\"\\nProcessing {img_path}...\")\n        result = apply_gradcam(model, img_path, class_names)\n        results.append(result)\n    return results\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****Gradcam on Aquascan","metadata":{}},{"cell_type":"code","source":"\n# Define the AquaScan-EfficientNet-Hybrid model class\nclass AquaScanEfficientNetHybrid(nn.Module):\n    def __init__(self, num_classes=7, model_name='efficientnet_b0', use_pretrained=True):\n        super(AquaScanEfficientNetHybrid, self).__init__()\n        \n        # Load pre-trained EfficientNet\n        if model_name == 'efficientnet_b0':\n            self.backbone = models.efficientnet_b0(weights='IMAGENET1K_V1' if use_pretrained else None)\n        elif model_name == 'efficientnet_b1':\n            self.backbone = models.efficientnet_b1(weights='IMAGENET1K_V1' if use_pretrained else None)\n        elif model_name == 'efficientnet_b2':\n            self.backbone = models.efficientnet_b2(weights='IMAGENET1K_V1' if use_pretrained else None)\n        else:\n            raise ValueError(f\"Unsupported model name: {model_name}\")\n        \n        # Extract features only \n        self.features = nn.Sequential(*list(self.backbone.children())[:-1])\n        \n        # Get the number of features from the backbone\n        if model_name == 'efficientnet_b0':\n            backbone_features = 1280\n        elif model_name == 'efficientnet_b1':\n            backbone_features = 1280\n        elif model_name == 'efficientnet_b2':\n            backbone_features = 1408\n        \n        \n        self.activation = nn.LeakyReLU(0.1, inplace=True)\n        \n        # Original AquaScan convolutional blocks with residual connections\n        # Block 1 with residual connection\n        self.conv_block1_main = nn.Sequential(\n            nn.Conv2d(backbone_features, backbone_features, kernel_size=3, padding=1),\n            nn.BatchNorm2d(backbone_features),\n            self.activation,\n            nn.Conv2d(backbone_features, backbone_features, kernel_size=3, padding=1),\n            nn.BatchNorm2d(backbone_features),\n            nn.Dropout(0.1)\n        )\n        \n        # Block 2 with residual connection\n        self.conv_block2_main = nn.Sequential(\n            nn.Conv2d(backbone_features, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            self.activation,\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.Dropout(0.1)\n        )\n        self.conv_block2_shortcut = nn.Sequential(\n            nn.Conv2d(backbone_features, 512, kernel_size=1, stride=1),\n            nn.BatchNorm2d(512)\n        )\n        \n        # Global Average Pooling\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        \n        # Fully Connected Blocks with more capacity\n        self.fc_block = nn.Sequential(\n            nn.Linear(512, 1024),\n            self.activation,\n            nn.Dropout(0.3),\n            nn.Linear(1024, 512),\n            self.activation,\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            self.activation,\n            nn.Dropout(0.3)\n        )\n        \n        # Output Layer\n        self.output_layer = nn.Linear(256, num_classes)\n    \n    def forward(self, x):\n        # Get features from EfficientNet backbone\n        x = self.features(x)\n        \n        # Block 1 with residual connection\n        identity = x\n        x = self.conv_block1_main(x)\n        x = x + identity  # Residual connection\n        x = self.activation(x)\n        \n        # Block 2 with residual connection\n        identity = self.conv_block2_shortcut(x)\n        x = self.conv_block2_main(x)\n        x = x + identity  # Residual connection\n        x = self.activation(x)\n        \n        # Global Average Pooling\n        x = self.gap(x)\n        x = x.view(x.size(0), -1)\n        \n        # Fully connected layers\n        x = self.fc_block(x)\n        x = self.output_layer(x)\n        \n        return x\n\n# Enhanced GradCAM class with better visualization\nclass GradCAM:\n    def __init__(self, model, target_layer):\n        self.model = model\n        self.target_layer = target_layer\n        self.gradients = None\n        self.activations = None\n        \n        # Register hooks\n        def forward_hook(module, input, output):\n            self.activations = output.detach()\n        \n        def backward_hook(module, grad_input, grad_output):\n            self.gradients = grad_output[0].detach()\n        \n        # Register hooks\n        self.target_layer.register_forward_hook(forward_hook)\n        self.target_layer.register_backward_hook(backward_hook)\n        \n        # Set model to evaluation mode\n        self.model.eval()\n    \n    def __call__(self, input_image, target_class=None):\n        # Forward pass\n        output = self.model(input_image)\n        \n        # Get the class index\n        if target_class is None:\n            pred = output.argmax(dim=1)[0]\n            class_idx = pred.item()\n        else:\n            class_idx = target_class\n        \n        # Get the prediction probability\n        pred_prob = F.softmax(output, dim=1)[0, class_idx].item()\n        \n        # Zero gradients\n        self.model.zero_grad()\n        \n        # Backward pass with the target class\n        target = output[0, class_idx]\n        target.backward()\n        \n        # Get the gradients and activations\n        gradients = self.gradients\n        activations = self.activations\n        \n        # Global average pooling of gradients\n        weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n        \n        # Weighted sum of activation maps\n        cam = torch.sum(weights * activations, dim=1, keepdim=True)\n        \n        # ReLU to keep only positive contributions\n        cam = F.relu(cam)\n        \n        # Normalize the CAM\n        cam = F.interpolate(cam, size=input_image.shape[2:], mode='bilinear', align_corners=False)\n        cam = cam - cam.min()\n        cam = cam / (cam.max() + 1e-8)\n        \n        # Convert to numpy\n        heatmap = cam[0, 0].cpu().numpy()\n        \n        return heatmap, class_idx, pred_prob\n\n# Function to preprocess images\ndef preprocess_image(image_path, transform=None):\n    try:\n        # Load image\n        img = Image.open(image_path).convert('RGB')\n        original_img = np.array(img)\n        \n        # Apply transformations\n        if transform is None:\n            transform = transforms.Compose([\n                transforms.Resize((224, 224)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n        \n        preprocessed_img = transform(img).unsqueeze(0)\n        \n        return preprocessed_img, original_img\n    except Exception as e:\n        print(f\"Error preprocessing image {image_path}: {e}\")\n        return None, None\n\n# Enhanced visualization function with better contrast and multiple views\ndef visualize_gradcam(original_img, heatmap, class_name, pred_prob, alpha=0.7, save_path=None):\n    \n    heatmap_eq = cv2.equalizeHist(np.uint8(255 * heatmap)) / 255.0\n    \n    # Convert heatmap to RGB with JET colormap (red = high activation)\n    heatmap_rgb = cv2.applyColorMap(np.uint8(255 * heatmap_eq), cv2.COLORMAP_JET)\n    heatmap_rgb = cv2.cvtColor(heatmap_rgb, cv2.COLOR_BGR2RGB)\n    \n    # Resize heatmap to match original image\n    heatmap_rgb = cv2.resize(heatmap_rgb, (original_img.shape[1], original_img.shape[0]))\n    \n    # Overlay heatmap on original image with higher alpha for better visibility\n    superimposed_img = heatmap_rgb * alpha + original_img * (1 - alpha)\n    superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)\n    \n    # Create a more comprehensive visualization with 4 panels\n    fig, ax = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # Original image\n    ax[0, 0].imshow(original_img)\n    ax[0, 0].set_title('Original Image')\n    ax[0, 0].axis('off')\n    \n    # Heatmap only\n    ax[0, 1].imshow(heatmap_eq, cmap='jet')\n    ax[0, 1].set_title('GradCAM Heatmap')\n    ax[0, 1].axis('off')\n    \n    # Overlaid image with high transparency\n    overlay_light = heatmap_rgb * 0.3 + original_img * 0.7\n    overlay_light = np.clip(overlay_light, 0, 255).astype(np.uint8)\n    ax[1, 0].imshow(overlay_light)\n    ax[1, 0].set_title('Subtle Overlay (30%)')\n    ax[1, 0].axis('off')\n    \n    # Overlaid image with stronger effect\n    ax[1, 1].imshow(superimposed_img)\n    ax[1, 1].set_title(f'Strong Overlay (70%): {class_name} ({pred_prob:.2%})')\n    ax[1, 1].axis('off')\n    \n    plt.tight_layout()\n    \n    # Save the visualization if a path is provided\n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    \n    plt.show()\n    \n    return superimposed_img\n\n# Function to get available target layers from the model\ndef get_target_layers(model):\n    target_layers = {\n        \"backbone_last\": model.features[-1],\n        \"conv_block1_first\": model.conv_block1_main[0],\n        \"conv_block1_last\": model.conv_block1_main[3],\n        \"conv_block2_first\": model.conv_block2_main[0],\n        \"conv_block2_last\": model.conv_block2_main[3]\n    }\n    return target_layers\n\n# Enhanced function to apply GradCAM to an image with better target layer selection\ndef apply_gradcam(model, image_path, class_names, target_layer_name=None, transform=None, target_class=None, save_path=None):\n    # Preprocess image\n    input_tensor, original_img = preprocess_image(image_path, transform)\n    \n    if input_tensor is None:\n        print(f\"Failed to load image: {image_path}\")\n        return None, None, None\n    \n    # Move to device\n    device = next(model.parameters()).device\n    input_tensor = input_tensor.to(device)\n    \n    # Get available target layers\n    target_layers = get_target_layers(model)\n    \n    # Select target layer\n    if target_layer_name is None or target_layer_name not in target_layers:\n        print(f\"Target layer '{target_layer_name}' not found. Using conv_block2_last.\")\n        target_layer = target_layers[\"conv_block2_last\"]\n        target_layer_name = \"conv_block2_last\"\n    else:\n        target_layer = target_layers[target_layer_name]\n    \n    print(f\"Using target layer: {target_layer_name}\")\n    \n    # Initialize Grad-CAM\n    grad_cam = GradCAM(model, target_layer)\n    \n    # Generate heatmap\n    heatmap, class_idx, pred_prob = grad_cam(input_tensor, target_class)\n    \n    # Get class name\n    class_name = class_names[class_idx] if class_idx < len(class_names) else f\"Class {class_idx}\"\n    \n    # Create save path if needed\n    if save_path is None and target_layer_name is not None:\n        base_name = os.path.basename(image_path)\n        save_dir = \"gradcam_results\"\n        os.makedirs(save_dir, exist_ok=True)\n        save_path = os.path.join(save_dir, f\"{os.path.splitext(base_name)[0]}_{target_layer_name}.png\")\n    \n    # Visualize\n    superimposed_img = visualize_gradcam(original_img, heatmap, class_name, pred_prob, save_path=save_path)\n    \n    print(f\"Predicted class: {class_name} with probability {pred_prob:.2%}\")\n    \n    return superimposed_img, class_idx, pred_prob\n\n# Function to compare GradCAM results from different layers\ndef compare_layers(model, image_path, class_names, transform=None, target_class=None):\n    # Get available target layers\n    target_layers = get_target_layers(model)\n    \n    # Create a directory for saving results\n    save_dir = \"gradcam_layer_comparison\"\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Process each layer\n    results = {}\n    for layer_name, layer in target_layers.items():\n        print(f\"\\nProcessing layer: {layer_name}\")\n        save_path = os.path.join(save_dir, f\"{os.path.basename(image_path).split('.')[0]}_{layer_name}.png\")\n        result = apply_gradcam(\n            model=model,\n            image_path=image_path,\n            class_names=class_names,\n            target_layer_name=layer_name,\n            transform=transform,\n            target_class=target_class,\n            save_path=save_path\n        )\n        results[layer_name] = result\n    \n    return results\n\n# Function to process multiple images\ndef process_multiple_images(model, image_paths, class_names, target_layer_name=None):\n    results = []\n    for img_path in image_paths:\n        print(f\"\\nProcessing {img_path}...\")\n        result = apply_gradcam(model, img_path, class_names, target_layer_name)\n        results.append(result)\n    return results\n\n# Function to process all images in a directory\ndef process_directory(model, directory_path, class_names, target_layer_name=None, pattern=\"*.jpg\"):\n    # Find all images in the directory\n    image_paths = []\n    for ext in [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\"]:\n        image_paths.extend(glob.glob(os.path.join(directory_path, ext)))\n        image_paths.extend(glob.glob(os.path.join(directory_path, \"**\", ext), recursive=True))\n    \n    if not image_paths:\n        print(f\"No images found in {directory_path}\")\n        return []\n    \n    print(f\"Found {len(image_paths)} images to process\")\n    return process_multiple_images(model, image_paths, class_names, target_layer_name)\n\n# MAIN \nif __name__ == \"__main__\":\n    try:\n        #  Setting device\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"Using device: {device}\")\n\n        #  Creating model instance\n        model = AquaScanEfficientNetHybrid(num_classes=7)\n        \n        #  Saving model \n        model_path = \"/kaggle/input/aquascanfishdisease/pytorch/default/1/aquascan_model_20250412_101409.pth\"\n        try:\n            model.load_state_dict(torch.load(model_path, map_location=device))\n            print(f\"Model loaded successfully from {model_path}\")\n        except Exception as e:\n            print(f\"Error loading model: {e}\")\n            raise\n            \n        model = model.to(device)\n        model.eval()\n\n        # 4. The classes\n        class_names = [\n            'Bacterial Red disease',\n            'Bacterial diseases - Aeromoniasis',\n            'Bacterial gill disease', \n            'Fungal diseases Saprolegniasis', \n            'Healthy Fish', \n            'Parasitic diseases', \n            'Viral diseases White tail disease' \n            \n        ]\n\n        # 5. Appling GradCAM to an image with different layers for comparison\n        image_path = '/kaggle/input/fish-split/Split/val/Bacterial Red disease/Bacterial Red disease (156).jpg'\n        \n        # Comparing different layers to find which one gives the best visualization\n        print(\"\\nComparing different layers for GradCAM visualization...\")\n        compare_layers(model, image_path, class_names)\n        \n        \n        print(\"\\nGradCAM implementation for AquaScan-EfficientNet-Hybrid completed!\")\n        \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        import traceback\n        traceback.print_exc()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****Download files from working folder","metadata":{}},{"cell_type":"code","source":"import shutil\n\n# Specify the folder you want to download\nfolder_to_download = '/kaggle/working/val'  # Replace with the name of your folder\noutput_zip = 'val.zip'\n\n# Compress the folder into a zip file\nshutil.make_archive(output_zip.replace('.zip', ''), 'zip', folder_to_download)\n\n# If running on Kaggle, use this to download the zip file\nfrom IPython.display import FileLink\nFileLink(output_zip)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T10:33:31.051964Z","iopub.execute_input":"2025-04-12T10:33:31.052234Z","iopub.status.idle":"2025-04-12T10:33:31.116100Z","shell.execute_reply.started":"2025-04-12T10:33:31.052217Z","shell.execute_reply":"2025-04-12T10:33:31.115497Z"}},"outputs":[],"execution_count":null}]}